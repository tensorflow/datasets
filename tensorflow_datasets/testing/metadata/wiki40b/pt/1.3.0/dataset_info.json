{
  "citation": "@inproceedings{49029,\ntitle = {Wiki-40B: Multilingual Language Model Dataset},\nauthor = {Mandy Guo and Zihang Dai and Denny Vrandecic and Rami Al-Rfou},\nyear = {2020},\nbooktitle\t= {LREC 2020}\n}",
  "configDescription": "Wiki40B dataset for pt.",
  "configName": "pt",
  "description": "Clean-up text for 40+ Wikipedia languages editions of pages\ncorrespond to entities. The datasets have train/dev/test splits per language.\nThe dataset is cleaned up by page filtering to remove disambiguation pages,\nredirect pages, deleted pages, and non-entity pages. Each example contains the\nwikidata id of the entity, and the full Wikipedia article after page processing\nthat removes non-content sections and structured objects. The language models\ntrained on this corpus - including 41 monolingual models, and 2 multilingual\nmodels - can be found at https://tfhub.dev/google/collections/wiki40b-lm/1.",
  "fileFormat": "array_record",
  "location": {
    "urls": [
      "https://research.google/pubs/pub49029/"
    ]
  },
  "moduleName": "tensorflow_datasets.text.wiki40b",
  "name": "wiki40b",
  "splits": [
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "train",
      "numBytes": "1047472224",
      "shardLengths": [
        "50813",
        "50814",
        "50813",
        "50814",
        "50813",
        "50813",
        "50814",
        "50813"
      ]
    },
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "validation",
      "numBytes": "58178400",
      "shardLengths": [
        "22301"
      ]
    },
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "test",
      "numBytes": "58209793",
      "shardLengths": [
        "22693"
      ]
    }
  ],
  "version": "1.3.0"
}