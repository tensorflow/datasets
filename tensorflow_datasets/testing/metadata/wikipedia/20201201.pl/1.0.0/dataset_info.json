{"name":"wikipedia","description":"Wikipedia dataset containing cleaned articles of all languages.\nThe datasets are built from the Wikipedia dump\n(https://dumps.wikimedia.org/) with one split per language. Each example\ncontains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).","citation":"@ONLINE {wikidump,\n    author = \"Wikimedia Foundation\",\n    title  = \"Wikimedia Downloads\",\n    url    = \"https://dumps.wikimedia.org\"\n}","location":{"urls":["https://dumps.wikimedia.org"]},"splits":[{"name":"train","shardLengths":["55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159","55159"],"numBytes":"2639698398"}],"version":"1.0.0","redistributionInfo":{"license":"This work is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA."},"downloadSize":"2124891082","configName":"20201201.pl","configDescription":"Wikipedia dataset for pl, parsed from 20201201 dump.","moduleName":"tensorflow_datasets.text.wikipedia","fileFormat":"array_record"}