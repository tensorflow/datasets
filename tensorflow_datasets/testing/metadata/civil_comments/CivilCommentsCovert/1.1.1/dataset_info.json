{
  "citation": "@article{DBLP:journals/corr/abs-1903-04561,\n  author    = {Daniel Borkan and\n               Lucas Dixon and\n               Jeffrey Sorensen and\n               Nithum Thain and\n               Lucy Vasserman},\n  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text\n               Classification},\n  journal   = {CoRR},\n  volume    = {abs/1903.04561},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1903.04561},\n  archivePrefix = {arXiv},\n  eprint    = {1903.04561},\n  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
  "configDescription": "\nThe CivilCommentsCovert set is a subset of CivilCommentsIdentities with ~20% of\nthe train and test splits further annotated for covert offensiveness, in\naddition to the toxicity and identity labels. Raters were asked to categorize\ncomments as one of explicitly, implicitly, not, or not sure if offensive, as\nwell as whether it contained different types of covert offensiveness. The full\nannotation procedure is detailed in a forthcoming paper at\nhttps://sites.google.com/corp/view/hciandnlp/accepted-papers.\n",
  "configName": "CivilCommentsCovert",
  "description": "This version of the CivilComments Dataset provides access to the primary\nseven labels that were annotated by crowd workers, the toxicity and other\ntags are a value between 0 and 1 indicating the fraction of annotators that\nassigned these attributes to the comment text.\n\nThe other tags are only available for a fraction of the input examples. They\nare currently ignored for the main dataset; the CivilCommentsIdentities set\nincludes those labels, but only consists of the subset of the data with them.\nThe other attributes that were part of the original CivilComments release are\nincluded only in the raw data. See the Kaggle documentation for more details\nabout the available features.\n\nThe comments in this dataset come from an archive of the Civil Comments\nplatform, a commenting plugin for independent news sites. These public comments\nwere created from 2015 - 2017 and appeared on approximately 50 English-language\nnews sites across the world. When Civil Comments shut down in 2017, they chose\nto make the public comments available in a lasting open archive to enable future\nresearch. The original data, published on figshare, includes the public comment\ntext, some associated metadata such as article IDs, timestamps and\ncommenter-generated \"civility\" labels, but does not include user ids. Jigsaw\nextended this dataset by adding additional labels for toxicity, identity\nmentions, as well as covert offensiveness. This data set is an exact replica of\nthe data released for the Jigsaw Unintended Bias in Toxicity Classification\nKaggle challenge. This dataset is released under CC0, as is the underlying\ncomment text.",
  "downloadSize": "417158441",
  "location": {
    "urls": [
      "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data"
    ]
  },
  "moduleName": "tensorflow_datasets.text.civil_comments",
  "name": "civil_comments",
  "splits": [
    {
      "name": "train",
      "numBytes": "79437837",
      "shardLengths": [
        "48074"
      ]
    },
    {
      "name": "test",
      "numBytes": "4041770",
      "shardLengths": [
        "2455"
      ]
    }
  ],
  "supervisedKeys": {
    "input": "text",
    "output": "toxicity"
  },
  "version": "1.1.1"
}