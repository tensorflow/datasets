{
  "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Ouguz, Barlas and Rinott, Ruty and Riedel,   Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}",
  "configDescription": "MLQA 'en' dev and test splits.",
  "configName": "en",
  "description": "MLQA (Multilingual Question Answering Dataset) is a benchmark dataset for evaluating multilingual question answering performance. The dataset consists of 7 languages: Arabic, German, Spanish, English, Hindi, Vietnamese, Chinese.",
  "downloadSize": "75719050",
  "fileFormat": "array_record",
  "location": {
    "urls": [
      "https://github.com/facebookresearch/MLQA"
    ]
  },
  "moduleName": "tensorflow_datasets.question_answering.mlqa",
  "name": "mlqa",
  "splits": [
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "test",
      "numBytes": "15054981",
      "shardLengths": [
        "11590"
      ]
    },
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "validation",
      "numBytes": "1433159",
      "shardLengths": [
        "1148"
      ]
    }
  ],
  "version": "1.0.0"
}