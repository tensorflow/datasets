{
  "citation": "@article{kwiatkowski-etal-2019-natural,\n    title = \"Natural Questions: A Benchmark for Question Answering Research\",\n    author = \"Kwiatkowski, Tom  and\n      Palomaki, Jennimaria  and\n      Redfield, Olivia  and\n      Collins, Michael  and\n      Parikh, Ankur  and\n      Alberti, Chris  and\n      Epstein, Danielle  and\n      Polosukhin, Illia  and\n      Devlin, Jacob  and\n      Lee, Kenton  and\n      Toutanova, Kristina  and\n      Jones, Llion  and\n      Kelcey, Matthew  and\n      Chang, Ming-Wei  and\n      Dai, Andrew M.  and\n      Uszkoreit, Jakob  and\n      Le, Quoc  and\n      Petrov, Slav\",\n    journal = \"Transactions of the Association for Computational Linguistics\",\n    volume = \"7\",\n    year = \"2019\",\n    address = \"Cambridge, MA\",\n    publisher = \"MIT Press\",\n    url = \"https://aclanthology.org/Q19-1026\",\n    doi = \"10.1162/tacl_a_00276\",\n    pages = \"452--466\",\n}\n\n@inproceedings{khashabi-etal-2020-unifiedqa,\n    title = \"{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System\",\n    author = \"Khashabi, Daniel  and\n      Min, Sewon  and\n      Khot, Tushar  and\n      Sabharwal, Ashish  and\n      Tafjord, Oyvind  and\n      Clark, Peter  and\n      Hajishirzi, Hannaneh\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.findings-emnlp.171\",\n    doi = \"10.18653/v1/2020.findings-emnlp.171\",\n    pages = \"1896--1907\",\n}\n\nNote that each UnifiedQA dataset has its own citation. Please see the source to\nsee the correct citation for each contained dataset.\"",
  "configDescription": "The NQ corpus contains questions from real users, and it requires QA\nsystems to read and comprehend an entire Wikipedia article that may or\nmay not contain the answer to the question. The inclusion of real user\nquestions, and the requirement that solutions should read an entire\npage to find the answer, cause NQ to be a more realistic and\nchallenging task than prior QA datasets. This version includes\nadditional paragraphs (obtained using the DPR retrieval engine) to\naugment each question.\n",
  "configName": "natural_questions_with_dpr_para_test",
  "description": "The UnifiedQA benchmark consists of 20 main question answering (QA) datasets\n(each may have multiple versions) that target different formats as well as\nvarious complex linguistic phenomena. These datasets are grouped into several\nformats/categories, including: extractive QA, abstractive QA, multiple-choice\nQA, and yes/no QA. Additionally, contrast sets are used for several datasets\n(denoted with \"contrast_sets_\"). These evaluation sets are expert-generated\nperturbations that deviate from the patterns common in the original dataset. For\nseveral datasets that do not come with evidence paragraphs, two variants are\nincluded: one where the datasets are used as-is and another that uses paragraphs\nfetched via an information retrieval system as additional evidence, indicated\nwith \"_ir\" tags.\n\nMore information can be found at: https://github.com/allenai/unifiedqa.",
  "downloadSize": "321851968",
  "fileFormat": "array_record",
  "location": {
    "urls": [
      "https://github.com/allenai/unifiedqa"
    ]
  },
  "moduleName": "tensorflow_datasets.text.unifiedqa.unifiedqa",
  "name": "unified_qa",
  "releaseNotes": {
    "1.0.0": "Initial release."
  },
  "splits": [
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "train",
      "numBytes": "304866189",
      "shardLengths": [
        "24169",
        "24169",
        "24169",
        "24169"
      ]
    },
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "test",
      "numBytes": "20698963",
      "shardLengths": [
        "6468"
      ]
    }
  ],
  "version": "1.0.0"
}