{
  "citation": "@inproceedings{mihaylov-etal-2018-suit,\n    title = \"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering\",\n    author = \"Mihaylov, Todor  and\n      Clark, Peter  and\n      Khot, Tushar  and\n      Sabharwal, Ashish\",\n    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n    month = oct # \"-\" # nov,\n    year = \"2018\",\n    address = \"Brussels, Belgium\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/D18-1260\",\n    doi = \"10.18653/v1/D18-1260\",\n    pages = \"2381--2391\",\n}\n\n@inproceedings{khashabi-etal-2020-unifiedqa,\n    title = \"{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System\",\n    author = \"Khashabi, Daniel  and\n      Min, Sewon  and\n      Khot, Tushar  and\n      Sabharwal, Ashish  and\n      Tafjord, Oyvind  and\n      Clark, Peter  and\n      Hajishirzi, Hannaneh\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.findings-emnlp.171\",\n    doi = \"10.18653/v1/2020.findings-emnlp.171\",\n    pages = \"1896--1907\",\n}\n\nNote that each UnifiedQA dataset has its own citation. Please see the source to\nsee the correct citation for each contained dataset.\"",
  "configDescription": "OpenBookQA aims to promote research in advanced question-answering,\nprobing a deeper understanding of both the topic (with salient facts\nsummarized as an open book, also provided with the dataset) and the\nlanguage it is expressed in. In particular, it contains questions that\nrequire multi-step reasoning, use of additional common and commonsense\nknowledge, and rich text comprehension. OpenBookQA is a new kind of\nquestion-answering dataset modeled after open book exams for assessing\nhuman understanding of a subject. This version includes paragraphs\nfetched via an information retrieval system as additional evidence.\n",
  "configName": "openbookqa_with_ir",
  "description": "The UnifiedQA benchmark consists of 20 main question answering (QA) datasets\n(each may have multiple versions) that target different formats as well as\nvarious complex linguistic phenomena. These datasets are grouped into several\nformats/categories, including: extractive QA, abstractive QA, multiple-choice\nQA, and yes/no QA. Additionally, contrast sets are used for several datasets\n(denoted with \"contrast_sets_\"). These evaluation sets are expert-generated\nperturbations that deviate from the patterns common in the original dataset. For\nseveral datasets that do not come with evidence paragraphs, two variants are\nincluded: one where the datasets are used as-is and another that uses paragraphs\nfetched via an information retrieval system as additional evidence, indicated\nwith \"_ir\" tags.\n\nMore information can be found at: https://github.com/allenai/unifiedqa.",
  "downloadSize": "6372961",
  "fileFormat": "array_record",
  "location": {
    "urls": [
      "https://github.com/allenai/unifiedqa"
    ]
  },
  "moduleName": "tensorflow_datasets.text.unifiedqa.unifiedqa",
  "name": "unified_qa",
  "releaseNotes": {
    "1.0.0": "Initial release."
  },
  "splits": [
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "train",
      "numBytes": "5474805",
      "shardLengths": [
        "4957"
      ]
    },
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "validation",
      "numBytes": "564099",
      "shardLengths": [
        "500"
      ]
    },
    {
      "filepathTemplate": "{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
      "name": "test",
      "numBytes": "548507",
      "shardLengths": [
        "500"
      ]
    }
  ],
  "version": "1.0.0"
}