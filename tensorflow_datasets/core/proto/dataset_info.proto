// Definitions for metadata related to whole datasets and its instances and
// splits.

syntax = "proto3";

package tensorflow_datasets;

import "feature.proto";
import "tensorflow_metadata/proto/v0/schema.proto";
import "tensorflow_metadata/proto/v0/statistics.proto";

option cc_enable_arenas = true;

// Message to represent location of a dataset, for now just has a url field, but
// can internally have folders etc.
message DatasetLocation {
  repeated string urls = 1;
}

// This is a serialization of tensorflow_datasets.core.SplitInfo -- this is
// supposed to encapsulate the information specific to a particular instance
// of this dataset, so attributes that are common to this dataset go directly
// in DatasetInfo (name, location, schema), but attributes specific to an
// instance go here.
message SplitInfo {
  // A string identifying this SplitInfo, i.e. "TRAIN", "TEST", "v18" etc.
  string name = 1;

  // The number of shards in this splits on-disk representation.
  int64 num_shards = 2;
  // The number of examples in each shard.
  repeated int64 shard_lengths = 4;
  // The number of bytes in the split.
  int64 num_bytes = 5;

  // The concrete statistics about this split.
  tensorflow.metadata.v0.DatasetFeatureStatistics statistics = 3;

  // Template where the files for this split can be found, relative to the main
  // builder folder of this dataset. Variables can be used that will be
  // automatically filled in. Some examples:
  //
  // {DATASET}_{SPLIT}.{FILEFORMAT}-{SHARD_INDEX}-of-{NUM_SHARDS}
  // This is the default TFDS filename template. Example of filenames:
  //   mnist_train.tfrecord-00000-of-00005
  //
  // my_data/{SPLIT}/{FILEFORMAT}_{SHARD_INDEX}
  // The data is in the subfolder 'my_data' in which there's another folder with
  // the name of the split. Files are named by the file format and use a
  // different shard template. Example filenames:
  //   my_data/train/tfrecord_00000
  //   my_data/train/tfrecord_00001
  //   my_data/test/tfrecord_00000
  //
  // Supported variables:
  // - {DATASET}: the name of the dataset.
  // - {SPLIT}: the split.
  // - {FILEFORMAT}: the fileformat used in the filename, e.g. tfrecord.
  // - {SHARD_INDEX}: the index of the current shard (starts with 0).
  // - {NUM_SHARDS}: total number of shards.
  // - {SHARD_X_OF_Y}: shard in xxxxx-of-yyyyy format.
  string filepath_template = 6;

  // Next available: 7.
}

// This message indicates which feature in the dataset schema is the input and
// which one is the output.
message SupervisedKeys {
  // Historical: Used before 2021-08-23
  // `input` and `output` are no longer used, but can still be loaded.
  // now everything goes in `tuple`
  string input = 1 [deprecated = true];
  string output = 2 [deprecated = true];

  Tuple tuple = 3;

  message Tuple {
    repeated Nest items = 1;
  }

  message Dict {
    map<string, Nest> dict = 1;
  }

  message Nest {
    oneof nest {
      string feature_key = 1;
      Tuple tuple = 2;
      Dict dict = 3;
    }
  }
}

// This message includes information for redistribution of the dataset.
message RedistributionInfo {
  // Text to be included in LICENSE file.
  string license = 1;
}

message DataSourceAccess {
  // The time when the data was accessed in milli seconds since unix epoch.
  int64 access_timestamp_ms = 1;

  oneof source {
    FileSystem file_system = 2;
    SqlQuery sql_query = 3;
    TfdsDatasetReference tfds_dataset = 4;
    Url url = 5;
  }
}

message FileSystem {
  // The path of the files that were read to generate the data. Can contain
  // wildcards, e.g. data-tfrecord-*.
  string path = 1;
}

message Url {
  // The URL of the resource that was accessed.
  string url = 1;

  // Optional checksum of the file that was downloaded.
  string checksum = 2;
}

message SqlQuery {
  // The SQL query that was executed to get the source data for the dataset.
  string sql_query = 1;
}

// A reference to a TFDS dataset.
message TfdsDatasetReference {
  // Name of the dataset, e.g. mnist.
  string name = 1;

  // The config of the variant.
  string config = 2;

  // The version of the dataset.
  string version = 3;

  // The root data dir that contains the dataset.
  string data_dir = 4;

  // The namespace of the dataset, e.g. huggingface. Note that we prefix by ds_
  // to avoid conflicts with reserved in keywords in languages like C++.
  string ds_namespace = 5;
}

// This is a serialization of tensorflow_datasets.core.DatasetInfo.
message DatasetInfo {
  string name = 1;
  string description = 2;

  // Version string of the dataset (ex: '1.0.5')
  string version = 9;

  // Release notes of the dataset
  map<string, string> release_notes = 18;

  // Config name of the dataset
  string config_name = 13;

  // Config description of the dataset
  string config_description = 14;

  // Config tags of the dataset
  repeated string config_tags = 21;

  // The structure and characteristics of the features of this dataset.
  Feature features = 19;

  // A citation string if one exists for this dataset.
  string citation = 3;

  // DEPRECATED
  // *Approximate* size in bytes of this dataset on disk.
  int64 size_in_bytes = 4 [deprecated = true];

  // Size in bytes of downloaded files.
  int64 download_size = 12;

  // Canonical location of the dataset.
  DatasetLocation location = 5;

  // DEPRECATED
  // Checksums of resources: URL to checksum of resource at URL.
  map<string, string> download_checksums = 10 [deprecated = true];

  // The schema of the dataset.
  tensorflow.metadata.v0.Schema schema = 6;

  // The information about the specific splits.
  repeated SplitInfo splits = 7;

  // If this is a supervised learning problem, then the input and output feature
  // can be specified using this.
  SupervisedKeys supervised_keys = 8;

  RedistributionInfo redistribution_info = 11;

  // Original module location of the dataset.
  // Allow to point to the original dataset source code in the documentation
  // even for datasets read from.
  string module_name = 15;

  // Specifies whether examples should be shuffled.
  bool disable_shuffling = 16;

  // File format used.
  // Use string to allow format extension without regenerating the proto.
  string file_format = 17;

  // The data that was used to generate this dataset.
  repeated DataSourceAccess data_source_accesses = 20;

  // Next available: 22
}
