{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mpii_human_pose.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GkN2UPpQQ1bT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MPII Human Pose with Tensorflow Datasets\n",
        "\n",
        "This notebook looks at exploring the [MPII Human Pose](http://human-pose.mpi-inf.mpg.de) dataset using `tensorflow_datasets`."
      ]
    },
    {
      "metadata": {
        "id": "6gbK6iWzRKb1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_datasets matplotlib scipy\n",
        "# scipy is required for dataset preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90TIbS_tKzgB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_datasets.human_pose.mpii import skeleton\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XLOJul4vPada",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "\n",
        "`tensorflow_datasets` will automatically download the relevant files, serialize them to disk and gather statistics. For `mpii_human_pose`, this is a releatively long process, though it is a one-time cost. After running to completion successfully, the relevant files will be stored and accessed immediately in future runs.\n",
        "\n",
        "Progress may appear to hang displaying `0 examples [00:00, ? examples/s]`. It may take several minutes.\n",
        "\n",
        "Due to the space requirements (the images are >12gb compressed, and the serialized form will take up about as much space again), you may have problems running this in a standard colab notebook. Check out [this post](https://research.google.com/colaboratory/local-runtimes.html) for instructions on how to run locally."
      ]
    },
    {
      "metadata": {
        "id": "gNSoLAvINOKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datasets, info = tfds.load('mpii_human_pose', with_info=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZYr2Ejs0TTeF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `info` object contains helpful information including (but not limited to) an overview of features."
      ]
    },
    {
      "metadata": {
        "id": "OF_xWo0kTRzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(info.features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFRNurbgi1vU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll set up some basic visualization functions. Feel free to skip the details and refer back when necessary.\n",
        "\n",
        "Box color codes:\n",
        "* Blue: rough bounding boxes based on center and scale\n",
        "* Red: head boxes with corresponding joint annotations\n",
        "* Green (rare): head boxes without corresponding annotations"
      ]
    },
    {
      "metadata": {
        "id": "-IvOfpOhNRBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "parent_indices = skeleton.s16.parent_indices\n",
        "num_joints = skeleton.s16.num_joints\n",
        "joint_names = skeleton.s16.joints\n",
        "colors = [\n",
        "    'red' if n.startswith('r_') else 'blue' if n.startswith('l_') else 'black'\n",
        "     for n in joint_names]\n",
        "\n",
        "\n",
        "def setup_fig(n=3):\n",
        "  fig, axes = plt.subplots(1, n, figsize=(n*6, 6),\n",
        "                           subplot_kw=dict(xticks=[], yticks=[]))\n",
        "  if n == 1:\n",
        "    axes = axes,\n",
        "  return fig, axes\n",
        "\n",
        "\n",
        "def label(key, value):\n",
        "  return 'unknown' if value == -1 else info.features[key].int2str(value)\n",
        "  \n",
        "\n",
        "def vis_skeleton(single_joints, single_visible, ax):\n",
        "  for child_index in range(num_joints):\n",
        "    parent_index = parent_indices[child_index]\n",
        "    if parent_index is None:\n",
        "      # root joint\n",
        "      continue\n",
        "    if single_visible[child_index] and single_visible[parent_index]:\n",
        "      child = single_joints[child_index]\n",
        "      parent = single_joints[parent_index]\n",
        "      ax.plot(\n",
        "          [child[0], parent[0]], [child[1], parent[1]],\n",
        "          color=colors[child_index])\n",
        "\n",
        "      \n",
        "def add_box(ymin, xmin, ymax, xmax, ax, **kwargs):\n",
        "  width = xmax - xmin\n",
        "  height = ymax - ymin\n",
        "  rect = patches.Rectangle(\n",
        "    (xmin, ymin), width, height,\n",
        "    linewidth=1, facecolor='none', **kwargs)\n",
        "  ax.add_patch(rect)\n",
        "\n",
        "\n",
        "def example_vis(\n",
        "    example, ax, show_heads=True, show_joints=True, show_scale_boxes=True,\n",
        "    multiple_targets=True):\n",
        "  \n",
        "  if not multiple_targets:\n",
        "    # add a leading dimension to simulate multiple targets\n",
        "    for k in 'head_box',:\n",
        "      if k in example:\n",
        "        example[k] = tf.expand_dims(example[k], axis=0)\n",
        "    targets = example['targets']\n",
        "    for k in 'joints', 'visible', 'center', 'scale':\n",
        "      targets[k] = tf.expand_dims(targets[k], axis=0)\n",
        "  \n",
        "  # print some metadata\n",
        "  if 'category' in example:\n",
        "    cat = example['category']\n",
        "    print('category: %s' % label('category', example['category']))\n",
        "    print('activity: %s' % label('activity', example['activity']))\n",
        "  if 'youtube_id' in example:\n",
        "    youtube_id = example['youtube_id'].numpy()\n",
        "    if youtube_id == \"UNK\":\n",
        "      youtube_link = youtube_id\n",
        "    else:\n",
        "      youtube_link = (\n",
        "          'https://www.youtube.com/watch?v=%s' % example['youtube_id'].numpy())\n",
        "    print('youtube_link: %s' % youtube_link)\n",
        "  if 'frame_sec' in example:\n",
        "    print('frame_sec: %d' % example['frame_sec'])\n",
        "\n",
        "  image = example['image'].numpy()\n",
        "  targets = example['targets']\n",
        "  joints = targets['joints']\n",
        "  visible = targets['visible']  # not all joints are visible\n",
        "  \n",
        "  visible_joints = tf.boolean_mask(joints, visible)\n",
        "  joints = joints.numpy()\n",
        "  visible = visible.numpy()\n",
        "  visible_joints = visible_joints.numpy()\n",
        "  ax.imshow(image)\n",
        "  \n",
        "  if show_joints:\n",
        "    visible_joints = joints[visible]\n",
        "    ax.scatter(visible_joints[..., 0], visible_joints[..., 1])\n",
        "    for single_joints, single_vis in zip(joints, visible):\n",
        "      vis_skeleton(single_joints, single_vis, ax)\n",
        "  \n",
        "  if show_heads:\n",
        "    head_boxes = example['head_box'].numpy()\n",
        "    num_skeletons = joints.shape[0]\n",
        "    # most examples have a head box for each skeleton, but some have more.\n",
        "    # heads with missing bodies are always at the end of the array\n",
        "    for box in head_boxes[:num_skeletons]:\n",
        "      add_box(*box, ax=ax, edgecolor='green')  # these heads have bodies\n",
        "\n",
        "    for box in head_boxes[num_skeletons:]:\n",
        "      add_box(*box, ax=ax, edgecolor='red')   # these don't - there may not be any\n",
        "  \n",
        "  if show_scale_boxes:\n",
        "    centers = targets['center'].numpy()\n",
        "    scales = targets['scale'].numpy()\n",
        "    ax.scatter(centers[:, 0], centers[:, 1], marker='+')\n",
        "  \n",
        "    # scale is defined rather arbitrarily, albeit consistently\n",
        "    for center, scale in zip(centers, scales):\n",
        "      xmin, ymin = center - 100 * scale\n",
        "      xmax, ymax = center + 100 * scale\n",
        "      add_box(ymin, xmin, ymax, xmax, ax=ax, edgecolor='blue')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RxVaDpujB7b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's have a quick look at what the basic dataset contains. Note the training dataset is shuffled by default during `load` so you can re-run the following cell for random results each time."
      ]
    },
    {
      "metadata": {
        "id": "xbyAg9xFjAVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = setup_fig()\n",
        "\n",
        "for example, ax in zip(datasets['train'], axes):\n",
        "  example_vis(example, ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O7iQ5sYDX4St",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The test dataset also contains center/scale annotations, but no joint/visibility/head boxes."
      ]
    },
    {
      "metadata": {
        "id": "PAGHztobUZtd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = setup_fig()\n",
        "\n",
        "for example, ax in zip(datasets['test'], axes):\n",
        "  example_vis(example, ax, show_heads=False, show_joints=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWH8RHkudaq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Single Target Inference\n",
        "\n",
        "Many models focus on single-target human pose estimation. Rather than reprocess the dataset (which will take time and up even more space), we can use [tf.data.Dataset.flat_map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map). In this case, we'll only consider sufficiently separated individuals. We'll [filter](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter) out those examples without any separated individuals before applying the `flat_map`.\n",
        "\n",
        "Alternatively, we could randomly sample  features associated with skeletons using [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map). This would lead to smaller epochs, as each image would only be used once. During training this would be fine, but this would make out test set incomplete."
      ]
    },
    {
      "metadata": {
        "id": "l6rEESwnVoVO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def crop_and_resize(example, target_width):\n",
        "  image = example['image']\n",
        "  targets = example['targets']\n",
        "\n",
        "  joints = targets['joints']\n",
        "  visible = targets['visible']\n",
        "  visible = tf.logical_or(visible, tf.reduce_all(tf.equal(joints, -1), axis=-1))\n",
        "  center = targets['center']\n",
        "  scale = targets['scale']\n",
        "\n",
        "  num_targets = tf.shape(joints)[0]\n",
        "\n",
        "  indices = example['separated_individuals']\n",
        "  num_targets = tf.size(indices)\n",
        "  joints = tf.gather(joints, indices)\n",
        "  visible = tf.gather(visible, indices)\n",
        "  center = tf.gather(center, indices)\n",
        "  scale = tf.gather(scale, indices)\n",
        "\n",
        "  radius = scale * 100\n",
        "  center =  tf.cast(center, tf.float32)\n",
        "\n",
        "  orig_dims = tf.cast(tf.shape(image)[:2], tf.float32)\n",
        "  expanded_radius = tf.expand_dims(radius, axis=-1)\n",
        "\n",
        "  center_yx = tf.reverse(center, [-1])\n",
        "\n",
        "  mins = center_yx - expanded_radius\n",
        "  maxs = center_yx + expanded_radius\n",
        "\n",
        "  boxes = tf.stack([mins, maxs], axis=-2)\n",
        "  boxes = boxes / orig_dims\n",
        "  boxes = tf.reshape(boxes, (-1, 4))\n",
        "\n",
        "  box_ind = tf.zeros(shape=(num_targets,), dtype=tf.int32)\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "  target_shape = tf.constant(\n",
        "      [target_width, target_width], dtype=tf.int32)\n",
        "\n",
        "  image = tf.image.crop_and_resize(\n",
        "    image, boxes, box_ind, target_shape)\n",
        "\n",
        "  new_center = tf.tile([[target_width // 2]], (num_targets, 2))\n",
        "  new_scale = tf.fill((num_targets,), target_width / 200)\n",
        "\n",
        "  target_width_f = tf.cast(target_width, tf.float32)\n",
        "  resize_factor = target_width_f / (2 * radius)\n",
        "\n",
        "  # Don't forget to adjust joints!\n",
        "  joints = tf.cast(joints, tf.float32)\n",
        "  joints = joints - tf.expand_dims(center, axis=-2)\n",
        "  joints = joints * tf.reshape(resize_factor, (-1, 1, 1))\n",
        "  joints = joints + tf.expand_dims(tf.cast(new_center, tf.float32), axis=-2)\n",
        "\n",
        "  return dict(\n",
        "    image=image,\n",
        "    targets=dict(\n",
        "      joints=joints,\n",
        "      visible=visible,\n",
        "      scale=new_scale,\n",
        "      center=new_center,\n",
        "    ),\n",
        "  )\n",
        "\n",
        "\n",
        "def flat_map_fn(example):\n",
        "  return tf.data.Dataset.from_tensor_slices(crop_and_resize(example, 128))\n",
        "\n",
        "\n",
        "filtered_dataset = datasets['train'].filter(\n",
        "    lambda x: tf.size(x['separated_individuals']) > 0)\n",
        "\n",
        "single_dataset = filtered_dataset.flat_map(flat_map_fn)\n",
        "\n",
        "fig, axes = setup_fig()\n",
        "\n",
        "for example, ax in zip(single_dataset, axes):\n",
        "  example['image'] /= 255\n",
        "  example_vis(example, ax, multiple_targets=False, show_heads=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdjdwBcxgXE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Finally, let's do some random color adjustments to augment our dataset and normalize the color values using [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)."
      ]
    },
    {
      "metadata": {
        "id": "VZYRU4t9idP5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_image(example, perturb=True):\n",
        "  image = example['image']\n",
        "  \n",
        "  if perturb:\n",
        "    # augment dataset by perturbing colors\n",
        "    image = tf.image.random_brightness(image, max_delta=63. / 255.)\n",
        "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "    image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
        "\n",
        "  example['image'] = tf.image.per_image_standardization(image)\n",
        "  return example\n",
        "\n",
        "\n",
        "fig, axes = setup_fig()\n",
        "  \n",
        "single_dataset = single_dataset.map(\n",
        "    preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "for example, ax in zip(single_dataset, axes):\n",
        "  image = example['image']\n",
        "#   renormalize for visualization only\n",
        "  image = image - tf.reduce_min(image, axis=(0, 1))\n",
        "  image = image / tf.reduce_max(image, axis=(0, 1))\n",
        "  example_vis(\n",
        "      example, ax, multiple_targets=False, show_heads=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5raunf4ejyhJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exercise: random data augmentation can improve network performance. Above we have randomly modified image properties. Play with the above `crop_and_resize_and_augment` function to randomly perturb the center/scale. If you're feeling brave, you can also try:\n",
        "* flipping the image left-right. You'll also have to change the `x`-values of your joints and reorder indices (left hands look different to right hands!). Check out `tensorflow_datasets.human_pos.skeleton.Skeleton.flip_left_right_indices`);\n",
        "* use a third-party image manipulation library like `cv2` along with `tf.py_function` to randomly _rotate_ your examples by a small angle. You'll also have to rotate your joints.\n",
        "* Some models target heatmaps rather than single points. Transform `joints` into a heatmap by setting values on a grid equal to a Gaussian centered at each joint position.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "keYkUZhbnCck",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make Something Awesome!\n",
        "\n",
        "Once you're happy with your input examples, use standard `tf.data.Dataset` tools, wire up a model, choose a loss function and away you go! Note `tfds.load` automatically applies `tf.data.Dataset.prefetch`, so no need to call it again."
      ]
    },
    {
      "metadata": {
        "id": "l9AZRof4mwNR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "single_dataset = single_dataset.repeat().shuffle(1024).batch(32)\n",
        "single_dataset = single_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# model = make_model(...)\n",
        "# model.fit(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxp-htTZyTsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The complete pipeline for single-target inference for convenience\n",
        "datasets = tfds.load('mpii_human_pose')\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = datasets['train'].flat_map(\n",
        "    flat_map_fn).map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "train_dataset = train_dataset.repeat().shuffle(1024).batch(32).prefetch(\n",
        "    AUTOTUNE)\n",
        "    \n",
        "test_dataset = datasets['test'].flat_map(flat_map_fn).map(\n",
        "    lambda image: preprocess_image(image, perturb=False),\n",
        "    num_parallel_calls=AUTOTUNE).batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtCcNZXwe6yj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.show()  # for users who download the `.py` file"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}