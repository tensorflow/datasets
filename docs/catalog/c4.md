<div itemscope itemtype="http://schema.org/Dataset">
  <div itemscope itemprop="includedInDataCatalog" itemtype="http://schema.org/DataCatalog">
    <meta itemprop="name" content="TensorFlow Datasets" />
  </div>
  <meta itemprop="name" content="c4" />
  <meta itemprop="description" content="A colossal, cleaned version of Common Crawl&#x27;s web crawl corpus.&#10;&#10;Based on Common Crawl dataset: https://commoncrawl.org&#10;&#10;To generate this dataset, please follow&#10;[the instructions from t5](https://github.com/google-research/text-to-text-transfer-transformer#c4).&#10;&#10;Due to the overhead of cleaning the dataset, it is recommend you prepare it with&#10;a distributed service like Cloud Dataflow. More info at&#10;https://www.tensorflow.org/datasets/beam_datasets.&#10;&#10;To use this dataset:&#10;&#10;```python&#10;import tensorflow_datasets as tfds&#10;&#10;ds = tfds.load(&#x27;c4&#x27;, split=&#x27;train&#x27;)&#10;for ex in ds.take(4):&#10;  print(ex)&#10;```&#10;&#10;See [the guide](https://www.tensorflow.org/datasets/overview) for more&#10;informations on [tensorflow_datasets](https://www.tensorflow.org/datasets).&#10;&#10;" />
  <meta itemprop="url" content="https://www.tensorflow.org/datasets/catalog/c4" />
  <meta itemprop="sameAs" content="https://github.com/google-research/text-to-text-transfer-transformer#datasets" />
  <meta itemprop="citation" content="@article{2019t5,&#10;  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},&#10;  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},&#10;  journal = {arXiv e-prints},&#10;  year = {2019},&#10;  archivePrefix = {arXiv},&#10;  eprint = {1910.10683},&#10;}" />
</div>

# `c4`


Note: This dataset has been updated since the last stable release. The new
versions and config marked with
<span class="material-icons" title="Available only in the tfds-nightly package">nights_stay</span>
are only available in the `tfds-nightly` package.

Warning: Manual download required. See instructions below.

*   **Description**:

A colossal, cleaned version of Common Crawl's web crawl corpus.

Based on Common Crawl dataset: https://commoncrawl.org

To generate this dataset, please follow
[the instructions from t5](https://github.com/google-research/text-to-text-transfer-transformer#c4).

Due to the overhead of cleaning the dataset, it is recommend you prepare it with
a distributed service like Cloud Dataflow. More info at
https://www.tensorflow.org/datasets/beam_datasets.

*   **Homepage**:
    [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)

*   **Source code**:
    [`tfds.text.C4`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/c4.py)

*   **Versions**:

    *   `2.2.0`: No release notes.
    *   `2.2.1`: No release notes.
    *   `2.3.0`: No release notes.
    *   `2.3.1`: No release notes.
    *   **`3.1.0`** (default)
        <span class="material-icons" title="Available only in the tfds-nightly package">nights_stay</span>:
        No release notes.

*   **Download size**: `Unknown size`

*   **Dataset size**: `Unknown size`

*   **Manual download instructions**: This dataset requires you to
    download the source data manually into `download_config.manual_dir`
    (defaults to `~/tensorflow_datasets/downloads/manual/`):<br/>
    You are using a C4 config that requires some files to be manually downloaded.
    For `c4/webtextlike`, download OpenWebText.zip from
    https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ.

*   **Auto-cached**
    ([documentation](https://www.tensorflow.org/datasets/performances#auto-caching)):
    Unknown

*   **Splits**:

Split | Examples
:---- | -------:

*   **Feature structure**:

```python
FeaturesDict({
    'content-length': Text(shape=(), dtype=tf.string),
    'content-type': Text(shape=(), dtype=tf.string),
    'text': Text(shape=(), dtype=tf.string),
    'timestamp': Text(shape=(), dtype=tf.string),
    'url': Text(shape=(), dtype=tf.string),
})
```

*   **Feature documentation**:

Feature        | Class        | Shape | Dtype     | Description
:------------- | :----------- | :---- | :-------- | :----------
               | FeaturesDict |       |           |
content-length | Text         |       | tf.string |
content-type   | Text         |       | tf.string |
text           | Text         |       | tf.string |
timestamp      | Text         |       | tf.string |
url            | Text         |       | tf.string |

*   **Supervised keys** (See
    [`as_supervised` doc](https://www.tensorflow.org/datasets/api_docs/python/tfds/load#args)):
    `None`

*   **Figure**
    ([tfds.show_examples](https://www.tensorflow.org/datasets/api_docs/python/tfds/visualization/show_examples)):
    Not supported.

*   **Examples**
    ([tfds.as_dataframe](https://www.tensorflow.org/datasets/api_docs/python/tfds/as_dataframe)):
    Missing.

*   **Citation**:

```
@article{2019t5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {arXiv e-prints},
  year = {2019},
  archivePrefix = {arXiv},
  eprint = {1910.10683},
}
```


## c4/en (default config)

*   **Config description**: English C4 dataset.

## c4/en.noclean

*   **Config description**: Disables all cleaning (deduplication, removal based
    on bad words, etc.)

## c4/realnewslike

*   **Config description**: Filters from the default config to only include
    content from the domains used in the 'RealNews' dataset (Zellers et al.,
    2019).

## c4/webtextlike

*   **Config description**: Filters from the default config to only include
    content from the URLs in OpenWebText
    (https://github.com/jcpeterson/openwebtext).

## c4/multilingual

*   **Config description**: Multilingual C4 (mC4) has 101 languages and is
    generated from 86 Common Crawl dumps.
