# php

References:

*   [Code](https://github.com/huggingface/datasets/blob/master/datasets/php)
*   [Huggingface](https://huggingface.co/datasets/php)


## fi-nl


Use the following command to load this dataset in TFDS:

```python
ds = tfds.load('huggingface:php/fi-nl')
```

*   **Description**:

```
A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.

23 languages, 252 bitexts
total number of files: 71,414
total number of tokens: 3.28M
total number of sentence fragments: 1.38M
```

*   **License**: No known license
*   **Version**: 1.0.0
*   **Splits**:

Split  | Examples
:----- | -------:
`'train'` | 27870

*   **Features**:

```json
{
    "id": {
        "dtype": "string",
        "id": null,
        "_type": "Value"
    },
    "translation": {
        "languages": [
            "fi",
            "nl"
        ],
        "id": null,
        "_type": "Translation"
    }
}
```



## it-ro


Use the following command to load this dataset in TFDS:

```python
ds = tfds.load('huggingface:php/it-ro')
```

*   **Description**:

```
A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.

23 languages, 252 bitexts
total number of files: 71,414
total number of tokens: 3.28M
total number of sentence fragments: 1.38M
```

*   **License**: No known license
*   **Version**: 1.0.0
*   **Splits**:

Split  | Examples
:----- | -------:
`'train'` | 28507

*   **Features**:

```json
{
    "id": {
        "dtype": "string",
        "id": null,
        "_type": "Value"
    },
    "translation": {
        "languages": [
            "it",
            "ro"
        ],
        "id": null,
        "_type": "Translation"
    }
}
```



## nl-sv


Use the following command to load this dataset in TFDS:

```python
ds = tfds.load('huggingface:php/nl-sv')
```

*   **Description**:

```
A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.

23 languages, 252 bitexts
total number of files: 71,414
total number of tokens: 3.28M
total number of sentence fragments: 1.38M
```

*   **License**: No known license
*   **Version**: 1.0.0
*   **Splits**:

Split  | Examples
:----- | -------:
`'train'` | 28079

*   **Features**:

```json
{
    "id": {
        "dtype": "string",
        "id": null,
        "_type": "Value"
    },
    "translation": {
        "languages": [
            "nl",
            "sv"
        ],
        "id": null,
        "_type": "Translation"
    }
}
```



## en-it


Use the following command to load this dataset in TFDS:

```python
ds = tfds.load('huggingface:php/en-it')
```

*   **Description**:

```
A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.

23 languages, 252 bitexts
total number of files: 71,414
total number of tokens: 3.28M
total number of sentence fragments: 1.38M
```

*   **License**: No known license
*   **Version**: 1.0.0
*   **Splits**:

Split  | Examples
:----- | -------:
`'train'` | 35538

*   **Features**:

```json
{
    "id": {
        "dtype": "string",
        "id": null,
        "_type": "Value"
    },
    "translation": {
        "languages": [
            "en",
            "it"
        ],
        "id": null,
        "_type": "Translation"
    }
}
```



## en-fr


Use the following command to load this dataset in TFDS:

```python
ds = tfds.load('huggingface:php/en-fr')
```

*   **Description**:

```
A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.

23 languages, 252 bitexts
total number of files: 71,414
total number of tokens: 3.28M
total number of sentence fragments: 1.38M
```

*   **License**: No known license
*   **Version**: 1.0.0
*   **Splits**:

Split  | Examples
:----- | -------:
`'train'` | 42222

*   **Features**:

```json
{
    "id": {
        "dtype": "string",
        "id": null,
        "_type": "Value"
    },
    "translation": {
        "languages": [
            "en",
            "fr"
        ],
        "id": null,
        "_type": "Translation"
    }
}
```


