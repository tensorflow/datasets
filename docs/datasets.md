<!-- auto-generated by tfds.scripts.document_datasets -->
# Datasets

## Usage

```python
# See all registered datasets
tfds.list_builders()

# Load a given dataset by name, along with the DatasetInfo
data, info = tfds.load("mnist", with_info=True)
train_data, test_data = data['train'], data['test']
assert isinstance(train_data, tf.data.Dataset)
assert info.features['label'].num_classes == 10
assert info.splits['train'].num_examples == 60000

# You can also access a builder directly
builder = tfds.builder("mnist")
assert builder.info.splits['train'].num_examples == 60000
builder.download_and_prepare()
datasets = builder.as_dataset()

# If you need NumPy arrays
np_datasets = tfds.as_numpy(datasets)
```

---

## All Datasets

* [`audio`](#audio)
  * [`"nsynth"`](#nsynth)
* [`image`](#image)
  * [`"caltech101"`](#caltech101)
  * [`"cats_vs_dogs"`](#cats_vs_dogs)
  * [`"celeb_a"`](#celeb_a)
  * [`"celeb_a_hq"`](#celeb_a_hq)
  * [`"cifar10"`](#cifar10)
  * [`"cifar100"`](#cifar100)
  * [`"coco2014"`](#coco2014)
  * [`"colorectal_histology"`](#colorectal_histology)
  * [`"colorectal_histology_large"`](#colorectal_histology_large)
  * [`"diabetic_retinopathy_detection"`](#diabetic_retinopathy_detection)
  * [`"emnist"`](#emnist)
  * [`"fashion_mnist"`](#fashion_mnist)
  * [`"horses_or_humans"`](#horses_or_humans)
  * [`"image_label_folder"`](#image_label_folder)
  * [`"imagenet2012"`](#imagenet2012)
  * [`"kmnist"`](#kmnist)
  * [`"lsun"`](#lsun)
  * [`"mnist"`](#mnist)
  * [`"omniglot"`](#omniglot)
  * [`"open_images_v4"`](#open_images_v4)
  * [`"quickdraw_bitmap"`](#quickdraw_bitmap)
  * [`"rock_paper_scissors"`](#rock_paper_scissors)
  * [`"svhn_cropped"`](#svhn_cropped)
  * [`"tf_flowers"`](#tf_flowers)

* [`structured`](#structured)
  * [`"titanic"`](#titanic)
* [`text`](#text)
  * [`"imdb_reviews"`](#imdb_reviews)
  * [`"lm1b"`](#lm1b)
  * [`"multi_nli"`](#multi_nli)
  * [`"squad"`](#squad)
* [`translate`](#translate)
  * [`"flores"`](#flores)
  * [`"ted_hrlr_translate"`](#ted_hrlr_translate)
  * [`"ted_multi_translate"`](#ted_multi_translate)
* [`video`](#video)
  * [`"bair_robot_pushing_small"`](#bair_robot_pushing_small)
  * [`"moving_mnist"`](#moving_mnist)
  * [`"starcraft_video"`](#starcraft_video)

---

## [`audio`](#audio)

### `"nsynth"`

The NSynth Dataset is an audio dataset containing ~300k musical notes, each
with a unique pitch, timbre, and envelope. Each note is annotated with three
additional pieces of information based on a combination of human evaluation
and heuristic algorithms:
 -Source: The method of sound production for the note's instrument.
 -Family: The high-level family of which the note's instrument is a member.
 -Qualities: Sonic qualities of the note.

The dataset is split into train, valid, and test sets, with no instruments
overlapping between the train set and the valid/test sets.


* URL: [https://g.co/magenta/nsynth-dataset](https://g.co/magenta/nsynth-dataset)
* `DatasetBuilder`: [`tfds.audio.nsynth.Nsynth`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/audio/nsynth.py)
* Version: `v1.0.0`
* Size: `73.07 GiB`

#### Features
```python
FeaturesDict({
    'audio': Tensor(shape=(64000,), dtype=tf.float32),
    'id': Tensor(shape=(), dtype=tf.string),
    'instrument': FeaturesDict({
        'family': ClassLabel(shape=(), dtype=tf.int64, num_classes=11),
        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=1006),
        'source': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),
    }),
    'pitch': ClassLabel(shape=(), dtype=tf.int64, num_classes=128),
    'qualities': FeaturesDict({
        'bright': Tensor(shape=(), dtype=tf.bool),
        'dark': Tensor(shape=(), dtype=tf.bool),
        'distortion': Tensor(shape=(), dtype=tf.bool),
        'fast_decay': Tensor(shape=(), dtype=tf.bool),
        'long_release': Tensor(shape=(), dtype=tf.bool),
        'multiphonic': Tensor(shape=(), dtype=tf.bool),
        'nonlinear_env': Tensor(shape=(), dtype=tf.bool),
        'percussive': Tensor(shape=(), dtype=tf.bool),
        'reverb': Tensor(shape=(), dtype=tf.bool),
        'tempo-synced': Tensor(shape=(), dtype=tf.bool),
    }),
    'velocity': ClassLabel(shape=(), dtype=tf.int64, num_classes=128),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |    305,979
TRAIN      |    289,205
VALID      |     12,678
TEST       |      4,096


#### Urls
 * [https://g.co/magenta/nsynth-dataset](https://g.co/magenta/nsynth-dataset)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@InProceedings{pmlr-v70-engel17a,
  title = 	 {Neural Audio Synthesis of Musical Notes with {W}ave{N}et Autoencoders},
  author = 	 {Jesse Engel and Cinjon Resnick and Adam Roberts and Sander Dieleman and Mohammad Norouzi and Douglas Eck and Karen Simonyan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1068--1077},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/engel17a/engel17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/engel17a.html},
}
```

---


## [`image`](#image)

### `"caltech101"`

Caltech-101 consists of pictures of objects belonging to 101 classes, plus
one `background clutter` class. Each image is labelled with a single object.
Each class contains roughly 40 to 800 images, totalling around 9k images.
Images are of variable sizes, with typical edge lengths of 200-300 pixels.
This version contains image-level labels only. The original dataset also
contains bounding boxes.


* URL: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)
* `DatasetBuilder`: [`tfds.image.caltech.Caltech101`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/caltech.py)
* Version: `v1.0.0`
* Size: `?? GiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'image/file_name': Text(shape=(), dtype=tf.string, encoder=None),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=102),
})
```


#### Statistics
None computed

#### Urls
 * [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{FeiFei2004LearningGV,
  title={Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},
  author={Li Fei-Fei and Rob Fergus and Pietro Perona},
  journal={Computer Vision and Pattern Recognition Workshop},
  year={2004},
}
```

---

### `"cats_vs_dogs"`

A large set of images of cats and dogs.There are 1738 corrupted images that are dropped.

* URL: [https://www.microsoft.com/en-us/download/details.aspx?id=54765](https://www.microsoft.com/en-us/download/details.aspx?id=54765)
* `DatasetBuilder`: [`tfds.image.cats_vs_dogs.CatsVsDogs`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/cats_vs_dogs.py)
* Version: `v2.0.0`
* Size: `786.68 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TRAIN      |     23,262
ALL        |     23,262


#### Urls
 * [https://www.microsoft.com/en-us/download/details.aspx?id=54765](https://www.microsoft.com/en-us/download/details.aspx?id=54765)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@Inproceedings (Conference){asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization,
author = {Elson, Jeremy and Douceur, John (JD) and Howell, Jon and Saul, Jared},
title = {Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization},
booktitle = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},
year = {2007},
month = {October},
publisher = {Association for Computing Machinery, Inc.},
url = {https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/},
edition = {Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)},
}
```

---

### `"celeb_a"`

CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations, including
 - 10,177 number of identities,
 - 202,599 number of face images, and
 - 5 landmark locations, 40 binary attributes annotations per image.

The dataset can be employed as the training and test sets for the following computer vision tasks: face attribute recognition, face detection, and landmark (or facial part) localization.


* URL: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
* `DatasetBuilder`: [`tfds.image.celeba.CelebA`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/celeba.py)
* Version: `v0.3.0`
* Size: `1.38 GiB`

#### Features
```python
FeaturesDict({
    'attributes': FeaturesDict({
        '5_o_Clock_Shadow': Tensor(shape=(), dtype=tf.bool),
        'Arched_Eyebrows': Tensor(shape=(), dtype=tf.bool),
        'Attractive': Tensor(shape=(), dtype=tf.bool),
        'Bags_Under_Eyes': Tensor(shape=(), dtype=tf.bool),
        'Bald': Tensor(shape=(), dtype=tf.bool),
        'Bangs': Tensor(shape=(), dtype=tf.bool),
        'Big_Lips': Tensor(shape=(), dtype=tf.bool),
        'Big_Nose': Tensor(shape=(), dtype=tf.bool),
        'Black_Hair': Tensor(shape=(), dtype=tf.bool),
        'Blond_Hair': Tensor(shape=(), dtype=tf.bool),
        'Blurry': Tensor(shape=(), dtype=tf.bool),
        'Brown_Hair': Tensor(shape=(), dtype=tf.bool),
        'Bushy_Eyebrows': Tensor(shape=(), dtype=tf.bool),
        'Chubby': Tensor(shape=(), dtype=tf.bool),
        'Double_Chin': Tensor(shape=(), dtype=tf.bool),
        'Eyeglasses': Tensor(shape=(), dtype=tf.bool),
        'Goatee': Tensor(shape=(), dtype=tf.bool),
        'Gray_Hair': Tensor(shape=(), dtype=tf.bool),
        'Heavy_Makeup': Tensor(shape=(), dtype=tf.bool),
        'High_Cheekbones': Tensor(shape=(), dtype=tf.bool),
        'Male': Tensor(shape=(), dtype=tf.bool),
        'Mouth_Slightly_Open': Tensor(shape=(), dtype=tf.bool),
        'Mustache': Tensor(shape=(), dtype=tf.bool),
        'Narrow_Eyes': Tensor(shape=(), dtype=tf.bool),
        'No_Beard': Tensor(shape=(), dtype=tf.bool),
        'Oval_Face': Tensor(shape=(), dtype=tf.bool),
        'Pale_Skin': Tensor(shape=(), dtype=tf.bool),
        'Pointy_Nose': Tensor(shape=(), dtype=tf.bool),
        'Receding_Hairline': Tensor(shape=(), dtype=tf.bool),
        'Rosy_Cheeks': Tensor(shape=(), dtype=tf.bool),
        'Sideburns': Tensor(shape=(), dtype=tf.bool),
        'Smiling': Tensor(shape=(), dtype=tf.bool),
        'Straight_Hair': Tensor(shape=(), dtype=tf.bool),
        'Wavy_Hair': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Earrings': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Hat': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Lipstick': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Necklace': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Necktie': Tensor(shape=(), dtype=tf.bool),
        'Young': Tensor(shape=(), dtype=tf.bool),
    }),
    'image': Image(shape=(218, 178, 3), dtype=tf.uint8),
    'landmarks': FeaturesDict({
        'lefteye_x': Tensor(shape=(), dtype=tf.int64),
        'lefteye_y': Tensor(shape=(), dtype=tf.int64),
        'leftmouth_x': Tensor(shape=(), dtype=tf.int64),
        'leftmouth_y': Tensor(shape=(), dtype=tf.int64),
        'nose_x': Tensor(shape=(), dtype=tf.int64),
        'nose_y': Tensor(shape=(), dtype=tf.int64),
        'righteye_x': Tensor(shape=(), dtype=tf.int64),
        'righteye_y': Tensor(shape=(), dtype=tf.int64),
        'rightmouth_x': Tensor(shape=(), dtype=tf.int64),
        'rightmouth_y': Tensor(shape=(), dtype=tf.int64),
    }),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |    202,599
TRAIN      |    162,770
TEST       |     19,962
VALIDATION |     19,867


#### Urls
 * [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@inproceedings{conf/iccv/LiuLWT15,
  added-at = {2018-10-09T00:00:00.000+0200},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},
  booktitle = {ICCV},
  crossref = {conf/iccv/2015},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},
  interhash = {3f735aaa11957e73914bbe2ca9d5e702},
  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},
  isbn = {978-1-4673-8391-2},
  keywords = {dblp},
  pages = {3730-3738},
  publisher = {IEEE Computer Society},
  timestamp = {2018-10-11T11:43:28.000+0200},
  title = {Deep Learning Face Attributes in the Wild.},
  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},
  year = 2015
}
```

---

### `"celeb_a_hq"`

High-quality version of the CELEBA
dataset, consisting of 30000 images in 1024 x 1024 resolution.

WARNING: This dataset currently requires you to prepare images on your own.


* URL: [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)
* `DatasetBuilder`: [`tfds.image.celebahq.CelebAHq`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/celebahq.py)

`celeb_a_hq` is configured with `tfds.image.celebahq.CelebaHQConfig` and has the following
configurations predefined (defaults to the first one):

* `"1024"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 1024 x 1024 resolution

* `"512"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 512 x 512 resolution

* `"256"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 256 x 256 resolution

* `"128"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 128 x 128 resolution

* `"64"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 64 x 64 resolution

* `"32"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 32 x 32 resolution

* `"16"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 16 x 16 resolution

* `"8"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 8 x 8 resolution

* `"4"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 4 x 4 resolution

* `"2"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 2 x 2 resolution

* `"1"` (`v0.1.0`) (`Size: ?? GiB`): CelebaHQ images in 1 x 1 resolution


#### `"celeb_a_hq/1024"`

```python
FeaturesDict({
    'image': Image(shape=(1024, 1024, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/512"`

```python
FeaturesDict({
    'image': Image(shape=(512, 512, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/256"`

```python
FeaturesDict({
    'image': Image(shape=(256, 256, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/128"`

```python
FeaturesDict({
    'image': Image(shape=(128, 128, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/64"`

```python
FeaturesDict({
    'image': Image(shape=(64, 64, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/32"`

```python
FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/16"`

```python
FeaturesDict({
    'image': Image(shape=(16, 16, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/8"`

```python
FeaturesDict({
    'image': Image(shape=(8, 8, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/4"`

```python
FeaturesDict({
    'image': Image(shape=(4, 4, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/2"`

```python
FeaturesDict({
    'image': Image(shape=(2, 2, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"celeb_a_hq/1"`

```python
FeaturesDict({
    'image': Image(shape=(1, 1, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
})
```




#### Statistics
Split  | Examples
:----- | ---:
TRAIN      |     30,000
ALL        |     30,000


#### Urls
 * [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{DBLP:journals/corr/abs-1710-10196,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  journal   = {CoRR},
  volume    = {abs/1710.10196},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10196},
  archivePrefix = {arXiv},
  eprint    = {1710.10196},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-10196},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

---

### `"cifar10"`

The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

* URL: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
* `DatasetBuilder`: [`tfds.image.cifar.Cifar10`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/cifar.py)
* Version: `v1.0.2`
* Size: `162.17 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     60,000
TRAIN      |     50,000
TEST       |     10,000


#### Urls
 * [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}
```

---

### `"cifar100"`

This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).

* URL: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
* `DatasetBuilder`: [`tfds.image.cifar.Cifar100`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/cifar.py)
* Version: `v1.3.1`
* Size: `160.71 MiB`

#### Features
```python
FeaturesDict({
    'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     60,000
TRAIN      |     50,000
TEST       |     10,000


#### Urls
 * [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}
```

---

### `"coco2014"`

COCO is a large-scale object detection, segmentation, and captioning dataset. This version contains images, bounding boxes and labels for the 2014 version.
Note:
 * Some images from the train and validation sets don't have annotations.
 * The test split don't have any annotations (only images).
 * Coco defines 91 classes but the data only had 80 classes.


* URL: [http://cocodataset.org/#home](http://cocodataset.org/#home)
* `DatasetBuilder`: [`tfds.image.coco.Coco2014`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/coco.py)
* Version: `v1.0.0`
* Size: `37.57 GiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
    'objects': SequenceDict({
        'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),
        'is_crowd': Tensor(shape=(), dtype=tf.bool),
        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80),
    }),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |    245,496
TRAIN      |     82,783
TEST2015   |     81,434
TEST       |     40,775
VALIDATION |     40,504


#### Urls
 * [http://cocodataset.org/#home](http://cocodataset.org/#home)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{DBLP:journals/corr/LinMBHPRDZ14,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  archivePrefix = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

---

### `"colorectal_histology"`

Classification of textures in colorectal cancer histology. Each example is a 150 x 150 x 3 RGB image of one of 8 classes.

* URL: [https://zenodo.org/record/53169#.XGZemKwzbmG](https://zenodo.org/record/53169#.XGZemKwzbmG)
* `DatasetBuilder`: [`tfds.image.colorectal_histology.ColorectalHistology`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/colorectal_histology.py)
* Version: `v0.0.1`
* Size: `246.14 MiB`

#### Features
```python
FeaturesDict({
    'filename': Text(shape=(), dtype=tf.string, encoder=None),
    'image': Image(shape=(150, 150, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=8),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TRAIN      |      5,000
ALL        |      5,000


#### Urls
 * [https://zenodo.org/record/53169#.XGZemKwzbmG](https://zenodo.org/record/53169#.XGZemKwzbmG)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{kather2016multi,
  title={Multi-class texture analysis in colorectal cancer histology},
  author={Kather, Jakob Nikolas and Weis, Cleo-Aron and Bianconi, Francesco and Melchers, Susanne M and Schad, Lothar R and Gaiser, Timo and Marx, Alexander and Z{"o}llner, Frank Gerrit},
  journal={Scientific reports},
  volume={6},
  pages={27988},
  year={2016},
  publisher={Nature Publishing Group}
}
```

---

### `"colorectal_histology_large"`

10 large 5000 x 5000 textured colorectal cancer histology images

* URL: [https://zenodo.org/record/53169#.XGZemKwzbmG](https://zenodo.org/record/53169#.XGZemKwzbmG)
* `DatasetBuilder`: [`tfds.image.colorectal_histology.ColorectalHistologyLarge`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/colorectal_histology.py)
* Version: `v0.0.1`
* Size: `707.65 MiB`

#### Features
```python
FeaturesDict({
    'filename': Text(shape=(), dtype=tf.string, encoder=None),
    'image': Image(shape=(5000, 5000, 3), dtype=tf.uint8),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TEST       |         10
ALL        |         10


#### Urls
 * [https://zenodo.org/record/53169#.XGZemKwzbmG](https://zenodo.org/record/53169#.XGZemKwzbmG)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{kather2016multi,
  title={Multi-class texture analysis in colorectal cancer histology},
  author={Kather, Jakob Nikolas and Weis, Cleo-Aron and Bianconi, Francesco and Melchers, Susanne M and Schad, Lothar R and Gaiser, Timo and Marx, Alexander and Z{"o}llner, Frank Gerrit},
  journal={Scientific reports},
  volume={6},
  pages={27988},
  year={2016},
  publisher={Nature Publishing Group}
}
```

---

### `"diabetic_retinopathy_detection"`

A large set of high-resolution retina images taken under a variety of imaging conditions.

* URL: [https://www.kaggle.com/c/diabetic-retinopathy-detection/data](https://www.kaggle.com/c/diabetic-retinopathy-detection/data)
* `DatasetBuilder`: [`tfds.image.diabetic_retinopathy_detection.DiabeticRetinopathyDetection`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/diabetic_retinopathy_detection.py)
* Version: `v1.0.0`
* Size: `?? GiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),
    'name': Text(shape=(), dtype=tf.string, encoder=None),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     88,712
TEST       |     53,576
TRAIN      |     35,126
SAMPLE     |         10


#### Urls
 * [https://www.kaggle.com/c/diabetic-retinopathy-detection/data](https://www.kaggle.com/c/diabetic-retinopathy-detection/data)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@ONLINE {kaggle-diabetic-retinopathy,
    author = "Kaggle and EyePacs",
    title  = "Kaggle Diabetic Retinopathy Detection",
    month  = "jul",
    year   = "2015",
    url    = "https://www.kaggle.com/c/diabetic-retinopathy-detection/data"
}
```

---

### `"emnist"`

The EMNIST dataset is a set of handwritten character digitsderived from the NIST Special Database 19 and converted toa 28x28 pixel image format and dataset structure that directlymatches the MNIST dataset.

* URL: [https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip](https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip)
* `DatasetBuilder`: [`tfds.image.mnist.EMNIST`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/mnist.py)

`emnist` is configured with `tfds.image.mnist.EMNISTConfig` and has the following
configurations predefined (defaults to the first one):

* `"byclass"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST ByClass:  814,255 characters. 62 unbalanced classes.

* `"bymerge"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST ByMerge: 814,255 characters. 47 unbalanced classes.

* `"balanced"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST Balanced:	131,600 characters. 47 balanced classes.

* `"letters"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST Letters:	103,600 characters. 26 balanced classes.

* `"digits"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST Digits:  280,000 characters. 10 balanced classes.

* `"mnist"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST MNIST:  70,000 characters. 10 balanced classes.

* `"test"` (`v1.0.1`) (`Size: ?? GiB`): EMNIST test data config.


#### `"emnist/byclass"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=62),
})
```



#### `"emnist/bymerge"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=47),
})
```



#### `"emnist/balanced"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=47),
})
```



#### `"emnist/letters"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=37),
})
```



#### `"emnist/digits"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```



#### `"emnist/mnist"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```



#### `"emnist/test"`

```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=62),
})
```




#### Statistics
None computed

#### Urls
 * [https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip](https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{cohen_afshar_tapson_schaik_2017, 
    title={EMNIST: Extending MNIST to handwritten letters}, 
    DOI={10.1109/ijcnn.2017.7966217}, 
    journal={2017 International Joint Conference on Neural Networks (IJCNN)}, 
    author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van}, 
    year={2017}
}
```

---

### `"fashion_mnist"`

Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.

* URL: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
* `DatasetBuilder`: [`tfds.image.mnist.FashionMNIST`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/mnist.py)
* Version: `v1.0.0`
* Size: `29.45 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     70,000
TRAIN      |     60,000
TEST       |     10,000


#### Urls
 * [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{DBLP:journals/corr/abs-1708-07747,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  archivePrefix = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

---

### `"horses_or_humans"`

A large set of images of horses and humans.

* URL: [http://laurencemoroney.com/horses-or-humans-dataset](http://laurencemoroney.com/horses-or-humans-dataset)
* `DatasetBuilder`: [`tfds.image.horses_or_humans.HorsesOrHumans`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/horses_or_humans.py)
* Version: `v1.0.0`
* Size: `153.59 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |      1,283
TRAIN      |      1,027
TEST       |        256


#### Urls
 * [http://laurencemoroney.com/horses-or-humans-dataset](http://laurencemoroney.com/horses-or-humans-dataset)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@ONLINE {horses_or_humans,
author = "Laurence Moroney",
title = "Horses or Humans Dataset",
month = "feb",
year = "2019",
url = "http://laurencemoroney.com/horses-or-humans-dataset"
}
```

---

### `"image_label_folder"`

Generic image classification dataset.

* URL: [<no known url>](<no known url>)
* `DatasetBuilder`: [`tfds.image.image_folder.ImageLabelFolder`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/image_folder.py)
* Version: `v1.0.0`
* Size: `?? GiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=None),
})
```


#### Statistics
None computed

#### Urls


#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`


---

### `"imagenet2012"`

ILSVRC 2012, aka ImageNet is an image dataset organized according to the
WordNet hierarchy. Each meaningful concept in WordNet, possibly described by
multiple words or word phrases, is called a "synonym set" or "synset". There are
more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In
ImageNet, we aim to provide on average 1000 images to illustrate each synset.
Images of each concept are quality-controlled and human-annotated. In its
completion, we hope ImageNet will offer tens of millions of cleanly sorted
images for most of the concepts in the WordNet hierarchy.


* URL: [http://image-net.org/](http://image-net.org/)
* `DatasetBuilder`: [`tfds.image.imagenet.Imagenet2012`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/imagenet.py)
* Version: `v2.0.0`
* Size: `?? GiB`

#### Features
```python
FeaturesDict({
    'file_name': Text(shape=(), dtype=tf.string, encoder=None),
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=1000),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |  1,331,167
TRAIN      |  1,281,167
VALIDATION |     50,000


#### Urls
 * [http://image-net.org/](http://image-net.org/)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}
```

---

### `"kmnist"`

Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images), provided in the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes, we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.

* URL: [http://codh.rois.ac.jp/kmnist/index.html.en](http://codh.rois.ac.jp/kmnist/index.html.en)
* `DatasetBuilder`: [`tfds.image.mnist.KMNIST`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/mnist.py)
* Version: `v1.0.0`
* Size: `20.26 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     70,000
TRAIN      |     60,000
TEST       |     10,000


#### Urls
 * [http://codh.rois.ac.jp/kmnist/index.html.en](http://codh.rois.ac.jp/kmnist/index.html.en)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@online{clanuwat2018deep,
  author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha},
  title        = {Deep Learning for Classical Japanese Literature},
  date         = {2018-12-03},
  year         = {2018},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  eprint       = {cs.CV/1812.01718},
}
```

---

### `"lsun"`

Large scale images showing different objects from given categories like bedroom, tower etc.

* URL: [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)
* `DatasetBuilder`: [`tfds.image.lsun.Lsun`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/lsun.py)

`lsun` is configured with `tfds.image.lsun.BuilderConfig` and has the following
configurations predefined (defaults to the first one):

* `"classroom"` (`v0.1.1`) (`Size: 3.06 GiB`): Classroom images.

* `"bedroom"` (`v0.1.1`) (`Size: 42.77 GiB`): Bedroom images.


#### `"lsun/classroom"`

```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
})
```



#### `"lsun/bedroom"`

```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |  3,033,342
TRAIN      |  3,033,042
VALIDATION |        300


#### Urls
 * [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{journals/corr/YuZSSX15,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
  biburl = {https://www.bibsonomy.org/bibtex/2446d4ffb99a5d7d2ab6e5417a12e195f/dblp},
  ee = {http://arxiv.org/abs/1506.03365},
  interhash = {3e9306c4ce2ead125f3b2ab0e25adc85},
  intrahash = {446d4ffb99a5d7d2ab6e5417a12e195f},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T15:08:59.000+0200},
  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1506.html#YuZSSX15},
  volume = {abs/1506.03365},
  year = 2015
}
```

---

### `"mnist"`

The MNIST database of handwritten digits.

* URL: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
* `DatasetBuilder`: [`tfds.image.mnist.MNIST`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/mnist.py)
* Version: `v1.0.0`
* Size: `11.06 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     70,000
TRAIN      |     60,000
TEST       |     10,000


#### Urls
 * [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
  volume={2},
  year={2010}
}
```

---

### `"omniglot"`

Omniglot data set for one-shot learning. This dataset contains 1623 different
handwritten characters from 50 different alphabets.


* URL: [https://github.com/brendenlake/omniglot/](https://github.com/brendenlake/omniglot/)
* `DatasetBuilder`: [`tfds.image.omniglot.Omniglot`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/omniglot.py)
* Version: `v1.0.0`
* Size: `17.95 MiB`

#### Features
```python
FeaturesDict({
    'alphabet': ClassLabel(shape=(), dtype=tf.int64, num_classes=50),
    'alphabet_char_id': Tensor(shape=(), dtype=tf.int64),
    'image': Image(shape=(105, 105, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=1623),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     38,300
TRAIN      |     19,280
TEST       |     13,180
SMALL2     |      3,120
SMALL1     |      2,720


#### Urls
 * [https://github.com/brendenlake/omniglot/](https://github.com/brendenlake/omniglot/)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}
```

---

### `"open_images_v4"`

Open Images is a dataset of ~9M images that have been annotated with image-level
 labels and object bounding boxes.

The training set of V4 contains 14.6M bounding boxes for 600 object classes on
1.74M images, making it the largest existing dataset with object location
annotations. The boxes have been largely manually drawn by professional
annotators to ensure accuracy and consistency. The images are very diverse and
often contain complex scenes with several objects (8.4 per image on average).
Moreover, the dataset is annotated with image-level labels spanning thousands of
classes.


* URL: [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)
* `DatasetBuilder`: [`tfds.image.open_images.OpenImagesV4`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/open_images.py)
* Version: `v0.1.0`
* Size: `565.11 GiB`

#### Features
```python
FeaturesDict({
    'bobjects': SequenceDict({
        'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),
        'is_depiction': Tensor(shape=(), dtype=tf.int8),
        'is_group_of': Tensor(shape=(), dtype=tf.int8),
        'is_inside': Tensor(shape=(), dtype=tf.int8),
        'is_occluded': Tensor(shape=(), dtype=tf.int8),
        'is_truncated': Tensor(shape=(), dtype=tf.int8),
        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=19995),
        'source': ClassLabel(shape=(), dtype=tf.int64, num_classes=6),
    }),
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
    'objects': SequenceDict({
        'confidence': Tensor(shape=(), dtype=tf.int32),
        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=19995),
        'source': ClassLabel(shape=(), dtype=tf.int64, num_classes=6),
    }),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |  1,910,098
TRAIN      |  1,743,042
TEST       |    125,436
VALIDATION |     41,620


#### Urls
 * [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{OpenImages,
  author = {Alina Kuznetsova and
            Hassan Rom and
            Neil Alldrin and
            Jasper Uijlings and
            Ivan Krasin and
            Jordi Pont-Tuset and
            Shahab Kamali and
            Stefan Popov and
            Matteo Malloci and
            Tom Duerig and
            Vittorio Ferrari},
  title = {The Open Images Dataset V4: Unified image classification,
           object detection, and visual relationship detection at scale},
  year = {2018},
  journal = {arXiv:1811.00982}
}
@article{OpenImages2,
  author = {Krasin, Ivan and
            Duerig, Tom and
            Alldrin, Neil and
            Ferrari, Vittorio
            and Abu-El-Haija, Sami and
            Kuznetsova, Alina and
            Rom, Hassan and
            Uijlings, Jasper and
            Popov, Stefan and
            Kamali, Shahab and
            Malloci, Matteo and
            Pont-Tuset, Jordi and
            Veit, Andreas and
            Belongie, Serge and
            Gomes, Victor and
            Gupta, Abhinav and
            Sun, Chen and
            Chechik, Gal and
            Cai, David and
            Feng, Zheyun and
            Narayanan, Dhyanesh and
            Murphy, Kevin},
  title = {OpenImages: A public dataset for large-scale multi-label and
           multi-class image classification.},
  journal = {Dataset available from
             https://storage.googleapis.com/openimages/web/index.html},
  year={2017}
}
```

---

### `"quickdraw_bitmap"`

The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The bitmap dataset contains these drawings converted from vector format into 28x28 grayscale images

* URL: [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)
* `DatasetBuilder`: [`tfds.image.quickdraw.QuickdrawBitmap`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/quickdraw.py)
* Version: `v1.0.0`
* Size: `36.82 GiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=345),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TRAIN      | 50,426,266
ALL        | 50,426,266


#### Urls
 * [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
A Neural Representation of Sketch Drawings, D. Ha and D. Eck, arXiv:1704.03477v4, 2017.
```

---

### `"rock_paper_scissors"`

Images of hands playing rock, paper, scissor game.

* URL: [http://laurencemoroney.com/rock-paper-scissors-dataset](http://laurencemoroney.com/rock-paper-scissors-dataset)
* `DatasetBuilder`: [`tfds.image.rock_paper_scissors.RockPaperScissors`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/rock_paper_scissors.py)
* Version: `v1.0.0`
* Size: `219.53 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(300, 300, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |      2,892
TRAIN      |      2,520
TEST       |        372


#### Urls
 * [http://laurencemoroney.com/rock-paper-scissors-dataset](http://laurencemoroney.com/rock-paper-scissors-dataset)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@ONLINE {rps,
author = "Laurence Moroney",
title = "Rock, Paper, Scissors Dataset",
month = "feb",
year = "2019",
url = "http://laurencemoroney.com/rock-paper-scissors-dataset"
}
```

---

### `"svhn_cropped"`

The Street View House Numbers (SVHN) Dataset is an image digit recognition dataset of over 600,000 digit images coming from real world data. Images are cropped to 32x32.

* URL: [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)
* `DatasetBuilder`: [`tfds.image.svhn.SvhnCropped`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/svhn.py)
* Version: `v1.0.0`
* Size: `1.47 GiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |    630,420
EXTRA      |    531,131
TRAIN      |     73,257
TEST       |     26,032


#### Urls
 * [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@article{Netzer2011,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems ({NIPS})},
title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
year = {2011}
}
```

---

### `"tf_flowers"`

A large set of images of flowers

* URL: [http://download.tensorflow.org/example_images/flower_photos.tgz](http://download.tensorflow.org/example_images/flower_photos.tgz)
* `DatasetBuilder`: [`tfds.image.flowers.TFFlowers`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/flowers.py)
* Version: `v1.0.0`
* Size: `218.21 MiB`

#### Features
```python
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TRAIN      |      3,670
ALL        |      3,670


#### Urls
 * [http://download.tensorflow.org/example_images/flower_photos.tgz](http://download.tensorflow.org/example_images/flower_photos.tgz)

#### Supervised keys (for `as_supervised=True`)
`(u'image', u'label')`

#### Citation
```
@ONLINE {tfflowers,
author = "The TensorFlow Team",
title = "Flowers",
month = "jan",
year = "2019",
url = "http://download.tensorflow.org/example_images/flower_photos.tgz" }
```

---



## [`structured`](#structured)

### `"titanic"`

Dataset describing the survival status of individual passengers on the Titanic. Missing values in the original dataset are represented using ?. Float and int missing values are replaced with -1, string missing values are replaced with 'Unknown'.

* URL: [https://www.openml.org/d/40945](https://www.openml.org/d/40945)
* `DatasetBuilder`: [`tfds.structured.titanic.Titanic`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/structured/titanic.py)
* Version: `v1.0.0`
* Size: `114.98 KiB`

#### Features
```python
FeaturesDict({
    'features': FeaturesDict({
        'age': Tensor(shape=(), dtype=tf.float32),
        'boat': Tensor(shape=(), dtype=tf.string),
        'body': Tensor(shape=(), dtype=tf.int32),
        'cabin': Tensor(shape=(), dtype=tf.string),
        'embarked': ClassLabel(shape=(), dtype=tf.int64, num_classes=4),
        'fare': Tensor(shape=(), dtype=tf.float32),
        'home.dest': Tensor(shape=(), dtype=tf.string),
        'name': Tensor(shape=(), dtype=tf.string),
        'parch': Tensor(shape=(), dtype=tf.int32),
        'pclass': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),
        'sex': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
        'sibsp': Tensor(shape=(), dtype=tf.int32),
        'ticket': Tensor(shape=(), dtype=tf.string),
    }),
    'survived': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TRAIN      |      1,309
ALL        |      1,309


#### Urls
 * [https://www.openml.org/d/40945](https://www.openml.org/d/40945)

#### Supervised keys (for `as_supervised=True`)
`(u'features', u'survived')`

#### Citation
```
@ONLINE {titanic,
author = "Frank E. Harrell Jr., Thomas Cason",
title  = "Titanic dataset",
month  = "oct",
year   = "2017",
url    = "https://www.openml.org/d/40945"
}
```

---


## [`text`](#text)

### `"imdb_reviews"`

Large Movie Review Dataset.
This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing.

* URL: [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
* `DatasetBuilder`: [`tfds.text.imdb.IMDBReviews`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/imdb.py)

`imdb_reviews` is configured with `tfds.text.imdb.IMDBReviewsConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`) (`Size: 80.23 MiB`): Plain text

* `"bytes"` (`v0.0.1`) (`Size: 80.23 MiB`): Uses byte-level text encoding with `tfds.features.text.ByteTextEncoder`

* `"subwords8k"` (`v0.0.1`) (`Size: 80.23 MiB`): Uses `tfds.features.text.SubwordTextEncoder` with 8k vocab size

* `"subwords32k"` (`v0.0.1`) (`Size: 80.23 MiB`): Uses `tfds.features.text.SubwordTextEncoder` with 32k vocab size


#### `"imdb_reviews/plain_text"`

```python
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"imdb_reviews/bytes"`

```python
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



#### `"imdb_reviews/subwords8k"`

```python
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),
})
```



#### `"imdb_reviews/subwords32k"`

```python
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32650>),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |     50,000
TRAIN      |     25,000
TEST       |     25,000


#### Urls
 * [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)

#### Supervised keys (for `as_supervised=True`)
`(u'text', u'label')`

#### Citation
```
@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}
```

---

### `"lm1b"`

A benchmark corpus to be used for measuring progress in statistical language modeling. This has almost one billion words in the training data.


* URL: [http://www.statmt.org/lm-benchmark/](http://www.statmt.org/lm-benchmark/)
* `DatasetBuilder`: [`tfds.text.lm1b.Lm1b`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/lm1b.py)

`lm1b` is configured with `tfds.text.lm1b.Lm1bConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`) (`Size: 1.67 GiB`): Plain text

* `"bytes"` (`v0.0.1`) (`Size: 1.67 GiB`): Uses byte-level text encoding with `tfds.features.text.ByteTextEncoder`

* `"subwords8k"` (`v0.0.2`) (`Size: 1.67 GiB`): Uses `tfds.features.text.SubwordTextEncoder` with 8k vocab size

* `"subwords32k"` (`v0.0.2`) (`Size: 1.67 GiB`): Uses `tfds.features.text.SubwordTextEncoder` with 32k vocab size


#### `"lm1b/plain_text"`

```python
FeaturesDict({
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"lm1b/bytes"`

```python
FeaturesDict({
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



#### `"lm1b/subwords8k"`

```python
FeaturesDict({
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8189>),
})
```



#### `"lm1b/subwords32k"`

```python
FeaturesDict({
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32711>),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        | 30,607,716
TRAIN      | 30,301,028
TEST       |    306,688


#### Urls
 * [http://www.statmt.org/lm-benchmark/](http://www.statmt.org/lm-benchmark/)

#### Supervised keys (for `as_supervised=True`)
`(u'text', u'text')`

#### Citation
```
@article{DBLP:journals/corr/ChelbaMSGBK13,
  author    = {Ciprian Chelba and
               Tomas Mikolov and
               Mike Schuster and
               Qi Ge and
               Thorsten Brants and
               Phillipp Koehn},
  title     = {One Billion Word Benchmark for Measuring Progress in Statistical Language
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1312.3005},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.3005},
  archivePrefix = {arXiv},
  eprint    = {1312.3005},
  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChelbaMSGBK13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

---

### `"multi_nli"`

The Multi-Genre Natural Language Inference (MultiNLI) corpus is a
crowd-sourced collection of 433k sentence pairs annotated with textual
entailment information. The corpus is modeled on the SNLI corpus, but differs in
that covers a range of genres of spoken and written text, and supports a
distinctive cross-genre generalization evaluation. The corpus served as the
basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.


* URL: [https://www.nyu.edu/projects/bowman/multinli/](https://www.nyu.edu/projects/bowman/multinli/)
* `DatasetBuilder`: [`tfds.text.multi_nli.MultiNLI`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/multi_nli.py)

`multi_nli` is configured with `tfds.text.multi_nli.MultiNLIConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`) (`Size: 216.34 MiB`): Plain text


#### `"multi_nli/plain_text"`

```python
FeaturesDict({
    'hypothesis': Text(shape=(), dtype=tf.string, encoder=None),
    'label': Text(shape=(), dtype=tf.string, encoder=None),
    'premise': Text(shape=(), dtype=tf.string, encoder=None),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |    402,702
TRAIN      |    392,702
VALIDATION |     10,000


#### Urls
 * [https://www.nyu.edu/projects/bowman/multinli/](https://www.nyu.edu/projects/bowman/multinli/)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of
               the North American Chapter of the
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}
```

---

### `"squad"`

Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.


* URL: [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
* `DatasetBuilder`: [`tfds.text.squad.Squad`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/squad.py)

`squad` is configured with `tfds.text.squad.SquadConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`) (`Size: 33.51 MiB`): Plain text

* `"bytes"` (`v0.0.1`) (`Size: 33.51 MiB`): Uses byte-level text encoding with `tfds.features.text.ByteTextEncoder`

* `"subwords8k"` (`v0.0.1`) (`Size: 33.51 MiB`): Uses `tfds.features.text.SubwordTextEncoder` with 8k vocab size

* `"subwords32k"` (`v0.0.2`) (`Size: 33.51 MiB`): Uses `tfds.features.text.SubwordTextEncoder` with 32k vocab size


#### `"squad/plain_text"`

```python
FeaturesDict({
    'context': Text(shape=(), dtype=tf.string, encoder=None),
    'first_answer': Text(shape=(), dtype=tf.string, encoder=None),
    'question': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"squad/bytes"`

```python
FeaturesDict({
    'context': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
    'first_answer': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
    'question': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



#### `"squad/subwords8k"`

```python
FeaturesDict({
    'context': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8190>),
    'first_answer': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8190>),
    'question': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8190>),
})
```



#### `"squad/subwords32k"`

```python
FeaturesDict({
    'context': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32953>),
    'first_answer': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32953>),
    'question': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32953>),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |     98,169
TRAIN      |     87,599
VALIDATION |     10,570


#### Urls
 * [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)

#### Supervised keys (for `as_supervised=True`)
`(u'', u'')`

#### Citation
```
@article{2016arXiv160605250R,
       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},
                 Konstantin and {Liang}, Percy},
        title = "{SQuAD: 100,000+ Questions for Machine Comprehension of Text}",
      journal = {arXiv e-prints},
         year = 2016,
          eid = {arXiv:1606.05250},
        pages = {arXiv:1606.05250},
archivePrefix = {arXiv},
       eprint = {1606.05250},
}
```

---


## [`translate`](#translate)

### `"flores"`

Evaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English.


* URL: [https://github.com/facebookresearch/flores/](https://github.com/facebookresearch/flores/)
* `DatasetBuilder`: [`tfds.translate.flores.Flores`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/translate/flores.py)

`flores` is configured with `tfds.translate.flores.FloresConfig` and has the following
configurations predefined (defaults to the first one):

* `"neen_plain_text"` (`v0.0.3`) (`Size: 984.65 KiB`): Translation dataset from ne to en, uses encoder plain_text.

* `"sien_plain_text"` (`v0.0.3`) (`Size: 984.65 KiB`): Translation dataset from si to en, uses encoder plain_text.


#### `"flores/neen_plain_text"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'ne': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"flores/sien_plain_text"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'si': Text(shape=(), dtype=tf.string, encoder=None),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |      5,664
VALIDATION |      2,898
TEST       |      2,766


#### Urls
 * [https://github.com/facebookresearch/flores/](https://github.com/facebookresearch/flores/)

#### Supervised keys (for `as_supervised=True`)
`(u'si', u'en')`

#### Citation
```
@misc{guzmn2019new,
    title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},
    author={Francisco Guzman and Peng-Jen Chen and Myle Ott and Juan Pino and Guillaume Lample and Philipp Koehn and Vishrav Chaudhary and Marc'Aurelio Ranzato},
    year={2019},
    eprint={1902.01382},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

---

### `"ted_hrlr_translate"`

Data sets derived from TED talk transcripts for comparing similar language pairs
where one is high resource and the other is low resource.


* URL: [https://github.com/neulab/word-embeddings-for-nmt](https://github.com/neulab/word-embeddings-for-nmt)
* `DatasetBuilder`: [`tfds.translate.ted_hrlr.TedHrlrTranslate`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/translate/ted_hrlr.py)

`ted_hrlr_translate` is configured with `tfds.translate.ted_hrlr.TedHrlrConfig` and has the following
configurations predefined (defaults to the first one):

* `"az_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from az to en in plain text.

* `"aztr_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from az_tr to en in plain text.

* `"be_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from be to en in plain text.

* `"beru_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from be_ru to en in plain text.

* `"es_to_pt"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from es to pt in plain text.

* `"fr_to_pt"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from fr to pt in plain text.

* `"gl_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from gl to en in plain text.

* `"glpt_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from gl_pt to en in plain text.

* `"he_to_pt"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from he to pt in plain text.

* `"it_to_pt"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from it to pt in plain text.

* `"pt_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from pt to en in plain text.

* `"ru_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from ru to en in plain text.

* `"ru_to_pt"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from ru to pt in plain text.

* `"tr_to_en"` (`v0.0.1`) (`Size: 124.94 MiB`): Translation dataset from tr to en in plain text.


#### `"ted_hrlr_translate/az_to_en"`

```python
Translation({
    'az': Text(shape=(), dtype=tf.string, encoder=None),
    'en': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/aztr_to_en"`

```python
Translation({
    'az_tr': Text(shape=(), dtype=tf.string, encoder=None),
    'en': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/be_to_en"`

```python
Translation({
    'be': Text(shape=(), dtype=tf.string, encoder=None),
    'en': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/beru_to_en"`

```python
Translation({
    'be_ru': Text(shape=(), dtype=tf.string, encoder=None),
    'en': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/es_to_pt"`

```python
Translation({
    'es': Text(shape=(), dtype=tf.string, encoder=None),
    'pt': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/fr_to_pt"`

```python
Translation({
    'fr': Text(shape=(), dtype=tf.string, encoder=None),
    'pt': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/gl_to_en"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'gl': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/glpt_to_en"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'gl_pt': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/he_to_pt"`

```python
Translation({
    'he': Text(shape=(), dtype=tf.string, encoder=None),
    'pt': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/it_to_pt"`

```python
Translation({
    'it': Text(shape=(), dtype=tf.string, encoder=None),
    'pt': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/pt_to_en"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'pt': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/ru_to_en"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'ru': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/ru_to_pt"`

```python
Translation({
    'pt': Text(shape=(), dtype=tf.string, encoder=None),
    'ru': Text(shape=(), dtype=tf.string, encoder=None),
})
```



#### `"ted_hrlr_translate/tr_to_en"`

```python
Translation({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'tr': Text(shape=(), dtype=tf.string, encoder=None),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |    191,524
TRAIN      |    182,450
TEST       |      5,029
VALIDATION |      4,045


#### Urls
 * [https://github.com/neulab/word-embeddings-for-nmt](https://github.com/neulab/word-embeddings-for-nmt)

#### Supervised keys (for `as_supervised=True`)
`(u'tr', u'en')`

#### Citation
```
@inproceedings{Ye2018WordEmbeddings,
  author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},
  title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},
  booktitle = {HLT-NAACL},
  year    = {2018},
  }
```

---

### `"ted_multi_translate"`

Massively multilingual (60 language) data set derived from TED Talk transcripts.
Each record consists of parallel arrays of language and text. Missing and
incomplete translations will be filtered out.


* URL: [https://github.com/neulab/word-embeddings-for-nmt](https://github.com/neulab/word-embeddings-for-nmt)
* `DatasetBuilder`: [`tfds.translate.ted_multi.TedMultiTranslate`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/translate/ted_multi.py)

`ted_multi_translate` is configured with `tfds.translate.ted_multi.BuilderConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.3`) (`Size: 335.91 MiB`): Plain text import of multilingual TED talk translations


#### `"ted_multi_translate/plain_text"`

```python
FeaturesDict({
    'talk_name': Text(shape=(), dtype=tf.string, encoder=None),
    'translations': TranslationVariableLanguages({
        'language': Text(shape=(), dtype=tf.string, encoder=None),
        'translation': Text(shape=(), dtype=tf.string, encoder=None),
    }),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |    271,360
TRAIN      |    258,098
TEST       |      7,213
VALIDATION |      6,049


#### Urls
 * [https://github.com/neulab/word-embeddings-for-nmt](https://github.com/neulab/word-embeddings-for-nmt)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@InProceedings{qi-EtAl:2018:N18-2,
  author    = {Qi, Ye  and  Sachan, Devendra  and  Felix, Matthieu  and  Padmanabhan, Sarguna  and  Neubig, Graham},
  title     = {When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {529--535},
  abstract  = {The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting.},
  url       = {http://www.aclweb.org/anthology/N18-2084}
}
```

---


## [`video`](#video)

### `"bair_robot_pushing_small"`

This data set contains roughly 44,000 examples of robot pushing motions, including one training set (train) and two test sets of previously seen (testseen) and unseen (testnovel) objects. This is the small 64x64 version.

* URL: [https://sites.google.com/site/brainrobotdata/home/push-dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset)
* `DatasetBuilder`: [`tfds.video.bair_robot_pushing.BairRobotPushingSmall`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/video/bair_robot_pushing.py)
* Version: `v1.0.0`
* Size: `30.06 GiB`

#### Features
```python
SequenceDict({
    'action': Tensor(shape=(4,), dtype=tf.float32),
    'endeffector_pos': Tensor(shape=(3,), dtype=tf.float32),
    'image_aux1': Image(shape=(64, 64, 3), dtype=tf.uint8),
    'image_main': Image(shape=(64, 64, 3), dtype=tf.uint8),
})
```


#### Statistics
Split  | Examples
:----- | ---:
ALL        |     43,520
TRAIN      |     43,264
TEST       |        256


#### Urls
 * [https://sites.google.com/site/brainrobotdata/home/push-dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@inproceedings{conf/nips/FinnGL16,
  added-at = {2016-12-16T00:00:00.000+0100},
  author = {Finn, Chelsea and Goodfellow, Ian J. and Levine, Sergey},
  biburl = {https://www.bibsonomy.org/bibtex/230073873b4fe43b314724b772d0f9256/dblp},
  booktitle = {NIPS},
  crossref = {conf/nips/2016},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike V. and Guyon, Isabelle and Garnett, Roman},
  ee = {http://papers.nips.cc/paper/6161-unsupervised-learning-for-physical-interaction-through-video-prediction},
  interhash = {2e6b416723704f4aa5ad0686ce5a3593},
  intrahash = {30073873b4fe43b314724b772d0f9256},
  keywords = {dblp},
  pages = {64-72},
  timestamp = {2016-12-17T11:33:40.000+0100},
  title = {Unsupervised Learning for Physical Interaction through Video Prediction.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2016.html#FinnGL16},
  year = 2016
}
```

---

### `"moving_mnist"`

Moving variant of MNIST database of handwritten digits. This is the
data used by the authors for reporting model performance. See
`tfds.video.moving_mnist.image_as_moving_sequence`
for generating training/validation data from the MNIST dataset.


* URL: [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)
* `DatasetBuilder`: [`tfds.video.moving_mnist.MovingMnist`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/video/moving_mnist.py)
* Version: `v0.1.0`
* Size: `781.25 MiB`

#### Features
```python
FeaturesDict({
    'image_sequence': Video(shape=(20, 64, 64, 1), dtype=tf.uint8, feature=Image(shape=(64, 64, 1), dtype=tf.uint8)),
})
```


#### Statistics
Split  | Examples
:----- | ---:
TEST       |     10,000
ALL        |     10,000


#### Urls
 * [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{DBLP:journals/corr/SrivastavaMS15,
  author    = {Nitish Srivastava and
               Elman Mansimov and
               Ruslan Salakhutdinov},
  title     = {Unsupervised Learning of Video Representations using LSTMs},
  journal   = {CoRR},
  volume    = {abs/1502.04681},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.04681},
  archivePrefix = {arXiv},
  eprint    = {1502.04681},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SrivastavaMS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

---

### `"starcraft_video"`

This data set contains videos generated from Starcraft.

* URL: [https://storage.googleapis.com/scv_dataset/README.html](https://storage.googleapis.com/scv_dataset/README.html)
* `DatasetBuilder`: [`tfds.video.starcraft.StarcraftVideo`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/video/starcraft.py)

`starcraft_video` is configured with `tfds.video.starcraft.StarcraftVideoConfig` and has the following
configurations predefined (defaults to the first one):

* `"brawl_64"` (`v0.1.2`) (`Size: 6.40 GiB`): Brawl map with 64x64 resolution.

* `"brawl_128"` (`v0.1.2`) (`Size: 20.76 GiB`): Brawl map with 128x128 resolution.

* `"collect_mineral_shards_64"` (`v0.1.2`) (`Size: 7.83 GiB`): CollectMineralShards map with 64x64 resolution.

* `"collect_mineral_shards_128"` (`v0.1.2`) (`Size: 24.83 GiB`): CollectMineralShards map with 128x128 resolution.

* `"move_unit_to_border_64"` (`v0.1.2`) (`Size: 1.77 GiB`): MoveUnitToBorder map with 64x64 resolution.

* `"move_unit_to_border_128"` (`v0.1.2`) (`Size: 5.75 GiB`): MoveUnitToBorder map with 128x128 resolution.

* `"road_trip_with_medivac_64"` (`v0.1.2`) (`Size: 2.48 GiB`): RoadTripWithMedivac map with 64x64 resolution.

* `"road_trip_with_medivac_128"` (`v0.1.2`) (`Size: 7.80 GiB`): RoadTripWithMedivac map with 128x128 resolution.


#### `"starcraft_video/brawl_64"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/brawl_128"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/collect_mineral_shards_64"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/collect_mineral_shards_128"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/move_unit_to_border_64"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/move_unit_to_border_128"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/road_trip_with_medivac_64"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



#### `"starcraft_video/road_trip_with_medivac_128"`

```python
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```




#### Statistics
Split  | Examples
:----- | ---:
ALL        |     14,000
TRAIN      |     10,000
VALIDATION |      2,000
TEST       |      2,000


#### Urls
 * [https://storage.googleapis.com/scv_dataset/README.html](https://storage.googleapis.com/scv_dataset/README.html)

#### Supervised keys (for `as_supervised=True`)
`None`

#### Citation
```
@article{DBLP:journals/corr/abs-1812-01717,
  author    = {Thomas Unterthiner and
               Sjoerd van Steenkiste and
               Karol Kurach and
               Rapha{"{e}}l Marinier and
               Marcin Michalski and
               Sylvain Gelly},
  title     = {Towards Accurate Generative Models of Video: {A} New Metric and
               Challenges},
  journal   = {CoRR},
  volume    = {abs/1812.01717},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.01717},
  archivePrefix = {arXiv},
  eprint    = {1812.01717},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-01717},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

---


