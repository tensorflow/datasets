{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIMvgrGMe7ZF"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n25wrPRbfCGc"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyGUj_q7IdfQ"
      },
      "source": [
        "# Dataset Collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpO0um1nez_q"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/datasets/dataset_collections\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/dataset_collections.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/dataset_collections.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/dataset_collections.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8AFT7CpSzBG"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Dataset collections provide a simple way to group together an arbitrary number\n",
        "of existing TFDS datasets, and to perform simple operations over them.\n",
        "\n",
        "They can be useful, for example, to group together different datasets related to the same task, or for easy [benchmarking](https://ruder.io/nlp-benchmarking/) of models over a fixed number of different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZjxBV9E79Fl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, install a few packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AnxnW65I_FC"
      },
      "outputs": [],
      "source": [
        "# Use tfds-nightly to ensure access to the latest features.\n",
        "!pip install -q tfds-nightly tensorflow\n",
        "!pip install -U conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81CCGS5R8GeV"
      },
      "source": [
        "Import TensorFlow and the Tensorflow Datasets package into your development environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hxMPT0wIu3f"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at0bMS_jIdjt"
      },
      "source": [
        "Dataset collections provide a simple way to group together an arbitrary number\n",
        "of existing datasets from Tensorflow Datasets (TFDS), and to perform simple operations over them.\n",
        "\n",
        "They can be useful, for example, to group together different datasets related to the same task, or for easy [benchmarking](https://ruder.io/nlp-benchmarking/) of models over a fixed number of different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLvkZBKwIdmL"
      },
      "source": [
        "## Find available dataset collections\n",
        "\n",
        "All dataset collection builders are a subclass of\n",
        "`tfds.core.dataset_collection_builder.DatasetCollection`.\n",
        "\n",
        "To get the list of available builders, use `tfds.list_dataset_collections()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R14uGGzKItDz"
      },
      "outputs": [],
      "source": [
        "tfds.list_dataset_collections()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpcq2AMvI5K1"
      },
      "source": [
        "## Load and inspect a dataset collection\n",
        "\n",
        "The easiest way of loading a dataset collection is to instantiate a `DatasetCollectionLoader` object using the [`tfds.dataset_collection`](https://www.tensorflow.org/datasets/api_docs/python/tfds/dataset_collection) command.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leIwyl9aI3WA"
      },
      "outputs": [],
      "source": [
        "collection_loader = tfds.dataset_collection('xtreme')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgjomybjY7qI"
      },
      "source": [
        "Specific dataset collection versions can be loaded following the same syntax as with TFDS datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyILkuYJY6ts"
      },
      "outputs": [],
      "source": [
        "collection_loader = tfds.dataset_collection('xtreme:1.0.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKOJ6CNQKG9S"
      },
      "source": [
        "A dataset collection loader can display information about the collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwk4PVDoKEAC"
      },
      "outputs": [],
      "source": [
        "collection_loader.print_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FlLLbwuLLTu"
      },
      "source": [
        "The dataset loader can also display information about the datasets contained in the collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxNJEie6K55T"
      },
      "outputs": [],
      "source": [
        "collection_loader.print_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxorc3kLwRj"
      },
      "source": [
        "### Loading datasets from a dataset collection\n",
        "\n",
        "The easiest way to load one dataset from a collection is to use a `DatasetCollectionLoader` object's `load_dataset` method, which loads the required dataset by calling [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n",
        "\n",
        "This call returns a dictionary of split names and the corresponding `tf.data.Dataset`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP1nRj4ILwb6"
      },
      "outputs": [],
      "source": [
        "splits = collection_loader.load_dataset(\"ner\")\n",
        "\n",
        "pprint.pprint(splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2spLEgN1Lwmm"
      },
      "source": [
        "`load_dataset` accepts the following optional parameters:\n",
        "\n",
        "* `split`: which split(s) to load. It accepts a single split (`split=\"test\"`) or a list of splits: (`split=[\"train\", \"test\"]`). If not specified, it will load all splits for the given dataset.\n",
        "* `loader_kwargs`: keyword arguments to be passed to the `tfds.load` function. Refer to the [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) documentation for a comprehensive overview of the different loading options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aClLU4eAh2oC"
      },
      "source": [
        "### Loading multiple datasets from a dataset collection\n",
        "\n",
        "The easiest way to load multiple datasets from a collection is to use the `DatasetCollectionLoader` object's `load_datasets` method, which loads the required datasets by calling [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n",
        "\n",
        "It returns a dictionary of dataset names, each one of which is associated with a dictionary of split names and the corresponding `tf.data.Dataset`s, as in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEG5744Oh2vQ"
      },
      "outputs": [],
      "source": [
        "datasets = collection_loader.load_datasets(['xnli', 'bucc'])\n",
        "\n",
        "pprint.pprint(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0kNqwsiN1Y"
      },
      "source": [
        "The `load_all_datasets` method loads *all* available datasets for a given collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX-M3xcjiM35"
      },
      "outputs": [],
      "source": [
        "all_datasets = collection_loader.load_all_datasets()\n",
        "\n",
        "pprint.pprint(all_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXxVztK5kAHh"
      },
      "source": [
        "The `load_datasets` method accepts the following optional parameters:\n",
        "\n",
        "* `split`: which split(s) to load. It accepts a single split `(split=\"test\")` or a list of splits: `(split=[\"train\", \"test\"])`. If not specified, it will load all splits for the given dataset.\n",
        "* `loader_kwargs`: keyword arguments to be passed to the `tfds.load` function. Refer to the [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) documentation for a comprehensive overview of the different loading options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4JoreSHfcKZ"
      },
      "source": [
        "### Specifying `loader_kwargs`\n",
        "\n",
        "The `loader_kwargs` are optional keyword arguments to be passed to the [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) function.\n",
        "They can be specified in three ways:\n",
        "\n",
        "1.  When initializing the `DatasetCollectionLoader` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjgZSIlvfcSP"
      },
      "outputs": [],
      "source": [
        "collection_loader = tfds.dataset_collection('xtreme', loader_kwargs=dict(split='train', batch_size=10, try_gcs=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJcEZl97Xj6Y"
      },
      "source": [
        "2.  Using `DatasetCollectioLoader`'s `set_loader_kwargs` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrysflp-k1d3"
      },
      "outputs": [],
      "source": [
        "collection_loader.set_loader_kwargs(dict(split='train', batch_size=10, try_gcs=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra-ZonhfXkLD"
      },
      "source": [
        "3.  As optional parameters to the `load_dataset`, `load_datasets` and `load_all_datasets` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHSu-8GnlGTk"
      },
      "outputs": [],
      "source": [
        "dataset = collection_loader.load_dataset('ner', loader_kwargs=dict(split='train', batch_size=10, try_gcs=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJDGoeAqmJAQ"
      },
      "source": [
        "### Feedback\n",
        "\n",
        "We are continuously trying to improve the dataset creation workflow, but can\n",
        "only do so if we are aware of the issues. Which issues, errors did you\n",
        "encountered while creating the dataset collection? Was there a part which was confusing,\n",
        "boilerplate or wasn't working the first time? Please share your feedback on\n",
        "[GitHub](https://github.com/tensorflow/datasets/issues)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dataset_collections.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
